{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 18: RNNs & LSTMs - Sequence Modeling Mastery\n",
        "\n",
        "**Welcome to Day 18 of your ML journey!** Today we dive into one of the most powerful architectures for sequential data: **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** networks. Building on your solid PyTorch foundation from Days 15-16, you'll now learn to build models that can understand temporal patterns, predict future values, and process sequential information with remarkable accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Master RNN/LSTM architecture and build production-ready time series prediction systems using PyTorch.\n",
        "\n",
        "**Topics Covered:**\n",
        "- RNN fundamentals: sequential processing and temporal dependencies\n",
        "- The vanishing gradient problem and why vanilla RNNs fail\n",
        "- LSTM architecture: gates, cell state, and long-term memory\n",
        "- GRU: simplified alternative to LSTM\n",
        "- Bidirectional RNNs: forward and backward context\n",
        "- Time series preprocessing and sequence preparation\n",
        "- Advanced techniques: stacked LSTMs, attention mechanisms\n",
        "- Production deployment and real-time inference\n",
        "\n",
        "**Real-World Impact:** RNNs and LSTMs power everything from stock market prediction to IoT sensor monitoring, from demand forecasting to medical diagnosis. By the end of today, you'll understand the technology behind these applications and be able to build your own sequence modeling systems.\n",
        "\n",
        "**Prerequisites:** Solid understanding of PyTorch fundamentals (Day 16), neural network basics (Day 15), and Python programming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Concept Overview: Understanding RNNs and LSTMs\n",
        "\n",
        "### What are Recurrent Neural Networks?\n",
        "\n",
        "**Recurrent Neural Networks (RNNs)** are specialized neural networks designed to process sequential data by maintaining a \"memory\" of previous inputs. Unlike feedforward networks that process each input independently, RNNs consider the temporal relationship between inputs.\n",
        "\n",
        "**The Core Intuition:**\n",
        "Think of RNNs like reading a book. As you read each word, you remember the context from previous words to understand the current sentence. Similarly, RNNs process each time step while remembering information from previous steps.\n",
        "\n",
        "**Why RNNs Excel at Sequential Data:**\n",
        "1. **Temporal Memory**: Maintains information across time steps\n",
        "2. **Parameter Sharing**: Same weights applied across all time steps\n",
        "3. **Variable Length**: Can handle sequences of different lengths\n",
        "4. **Context Awareness**: Current prediction depends on entire history\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **Financial Markets**: Stock price prediction, algorithmic trading\n",
        "- **IoT & Sensors**: Predictive maintenance, anomaly detection\n",
        "- **Healthcare**: Patient monitoring, drug discovery\n",
        "- **Energy**: Power demand forecasting, renewable energy prediction\n",
        "- **Manufacturing**: Quality control, supply chain optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RNN Architecture Deep Dive\n",
        "\n",
        "**Basic RNN Structure:**\n",
        "\n",
        "```\n",
        "Input Sequence: [x₁, x₂, x₃, ..., xₜ]\n",
        "Hidden States:  [h₁, h₂, h₃, ..., hₜ]\n",
        "Outputs:        [y₁, y₂, y₃, ..., yₜ]\n",
        "```\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "At each time step t:\n",
        "- **Hidden State**: hₜ = tanh(Wₕₕ × hₜ₋₁ + Wₓₕ × xₜ + bₕ)\n",
        "- **Output**: yₜ = Wₕᵧ × hₜ + bᵧ\n",
        "\n",
        "**Key Components:**\n",
        "1. **Input Layer**: Receives current time step data\n",
        "2. **Hidden Layer**: Maintains memory of previous states\n",
        "3. **Output Layer**: Produces prediction for current time step\n",
        "4. **Recurrent Connection**: Passes information to next time step\n",
        "\n",
        "**Unrolling Through Time:**\n",
        "RNNs can be \"unrolled\" to show how information flows through time:\n",
        "\n",
        "```\n",
        "Time Step 1: x₁ → h₁ → y₁\n",
        "Time Step 2: x₂ + h₁ → h₂ → y₂\n",
        "Time Step 3: x₃ + h₂ → h₃ → y₃\n",
        "```\n",
        "\n",
        "*Visual Suggestion: Create a diagram showing RNN unrolling with arrows indicating information flow through time*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Vanishing Gradient Problem\n",
        "\n",
        "**The Challenge:**\n",
        "Vanilla RNNs suffer from the \"vanishing gradient problem\" when processing long sequences. Gradients become exponentially smaller as they propagate backward through time, making it nearly impossible to learn long-term dependencies.\n",
        "\n",
        "**Mathematical Intuition:**\n",
        "When computing gradients through time using backpropagation:\n",
        "\n",
        "∂L/∂hₜ = ∂L/∂hₜ₊₁ × ∂hₜ₊₁/∂hₜ = ∂L/∂hₜ₊₁ × Wₕₕ × tanh'(hₜ)\n",
        "\n",
        "Since tanh'(x) ≤ 1 and Wₕₕ is typically < 1, gradients shrink exponentially:\n",
        "\n",
        "∂L/∂h₁ ≈ ∂L/∂hₜ × (Wₕₕ)ᵗ × ∏ᵢ tanh'(hᵢ)\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Short-term Memory**: RNNs can only remember recent information\n",
        "- **Training Instability**: Gradients become too small to update weights\n",
        "- **Poor Long-term Dependencies**: Cannot learn patterns spanning many time steps\n",
        "\n",
        "**Real-World Impact:**\n",
        "- Stock prediction over months/years fails\n",
        "- Sensor data with seasonal patterns struggles\n",
        "- Language modeling with long sentences fails\n",
        "\n",
        "*Visual Suggestion: Create a graph showing gradient magnitude decreasing exponentially over time steps*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM: The Solution to Vanishing Gradients\n",
        "\n",
        "**Long Short-Term Memory (LSTM)** networks solve the vanishing gradient problem through a sophisticated gating mechanism that can selectively remember or forget information.\n",
        "\n",
        "**LSTM Architecture Components:**\n",
        "\n",
        "1. **Cell State (Cₜ)**: The \"conveyor belt\" that carries information across time steps\n",
        "2. **Hidden State (hₜ)**: The \"working memory\" used for predictions\n",
        "3. **Gates**: Control mechanisms that decide what information to keep, forget, or add\n",
        "\n",
        "**The Three Gates:**\n",
        "\n",
        "**1. Forget Gate (fₜ)**: \"What should we forget?\"\n",
        "- fₜ = σ(Wf × [hₜ₋₁, xₜ] + bf)\n",
        "- Decides what information to discard from cell state\n",
        "\n",
        "**2. Input Gate (iₜ)**: \"What new information should we store?\"\n",
        "- iₜ = σ(Wi × [hₜ₋₁, xₜ] + bi)\n",
        "- C̃ₜ = tanh(WC × [hₜ₋₁, xₜ] + bC)\n",
        "- Decides what new information to add to cell state\n",
        "\n",
        "**3. Output Gate (oₜ)**: \"What should we output?\"\n",
        "- oₜ = σ(Wo × [hₜ₋₁, xₜ] + bo)\n",
        "- Controls what parts of cell state are output as hidden state\n",
        "\n",
        "**Cell State Update:**\n",
        "- Cₜ = fₜ × Cₜ₋₁ + iₜ × C̃ₜ\n",
        "- hₜ = oₜ × tanh(Cₜ)\n",
        "\n",
        "**Why LSTMs Work:**\n",
        "- **Selective Memory**: Can remember important information for very long periods\n",
        "- **Gradient Flow**: Cell state provides a \"highway\" for gradients to flow\n",
        "- **Adaptive Learning**: Gates learn what to remember/forget automatically\n",
        "\n",
        "*Visual Suggestion: Create a detailed LSTM cell diagram showing gates, cell state, and information flow*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GRU: Simplified Alternative to LSTM\n",
        "\n",
        "**Gated Recurrent Unit (GRU)** is a simplified version of LSTM that combines the forget and input gates into a single \"update gate\" while maintaining similar performance.\n",
        "\n",
        "**GRU Architecture:**\n",
        "\n",
        "**1. Reset Gate (rₜ)**: \"How much of the past should we ignore?\"\n",
        "- rₜ = σ(Wr × [hₜ₋₁, xₜ] + br)\n",
        "\n",
        "**2. Update Gate (zₜ)**: \"How much of the new information should we keep?\"\n",
        "- zₜ = σ(Wz × [hₜ₋₁, xₜ] + bz)\n",
        "\n",
        "**3. Candidate Hidden State:**\n",
        "- h̃ₜ = tanh(Wh × [rₜ × hₜ₋₁, xₜ] + bh)\n",
        "\n",
        "**4. Final Hidden State:**\n",
        "- hₜ = (1 - zₜ) × hₜ₋₁ + zₜ × h̃ₜ\n",
        "\n",
        "**GRU vs LSTM:**\n",
        "| Feature | LSTM | GRU |\n",
        "|---------|------|-----|\n",
        "| Parameters | More | Fewer |\n",
        "| Training Speed | Slower | Faster |\n",
        "| Memory Capacity | Higher | Lower |\n",
        "| Performance | Often better | Often comparable |\n",
        "\n",
        "**When to Use GRU:**\n",
        "- Limited computational resources\n",
        "- Smaller datasets\n",
        "- When LSTM performance is similar\n",
        "- Real-time applications requiring speed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bidirectional RNNs: Context from Both Directions\n",
        "\n",
        "**Bidirectional RNNs** process sequences in both forward and backward directions, allowing the model to use information from both past and future time steps.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Forward:  x₁ → x₂ → x₃ → x₄\n",
        "Backward: x₁ ← x₂ ← x₃ ← x₄\n",
        "Output:   Combine both directions\n",
        "```\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "- Forward hidden state: h⃗ₜ = f(W⃗ₓₕ × xₜ + W⃗ₕₕ × h⃗ₜ₋₁ + b⃗ₕ)\n",
        "- Backward hidden state: h⃖ₜ = f(W⃖ₓₕ × xₜ + W⃖ₕₕ × h⃖ₜ₊₁ + b⃖ₕ)\n",
        "- Combined output: yₜ = Wᵧₕ × [h⃗ₜ, h⃖ₜ] + bᵧ\n",
        "\n",
        "**Advantages:**\n",
        "- **Richer Context**: Uses information from entire sequence\n",
        "- **Better Performance**: Often outperforms unidirectional RNNs\n",
        "- **Pattern Recognition**: Can identify patterns that span the sequence\n",
        "\n",
        "**Limitations:**\n",
        "- **Not Real-time**: Requires entire sequence before prediction\n",
        "- **More Parameters**: Doubles the number of parameters\n",
        "- **Computational Cost**: More expensive to train and infer\n",
        "\n",
        "**When to Use Bidirectional:**\n",
        "- Offline analysis (not real-time)\n",
        "- Sequence classification tasks\n",
        "- When you have the complete sequence\n",
        "- Pattern recognition across the entire sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Code Demo: Building RNNs and LSTMs with PyTorch\n",
        "\n",
        "Let's dive into practical implementation! We'll start with a simple RNN and progressively build more sophisticated architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Environment Setup and Imports\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
