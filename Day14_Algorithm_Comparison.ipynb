{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 14: Algorithm Comparison Mini-Project\n",
        "\n",
        "**Week 2 - Comprehensive ML Model Comparison Practice Template**\n",
        "\n",
        "Welcome to Day 14 of your ML journey! Today we tackle one of the most critical decisions in machine learning: **Algorithm Selection**. Choosing the right algorithm can make the difference between a mediocre model and an exceptional one. you'll perform comprehensive model comparison, performance evaluation across multiple metrics, cross-validation techniques, and systematic algorithm selection strategies.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this project, you will be able to:\n",
        "\n",
        "1. **Apply multiple ML algorithms** to the same dataset\n",
        "2. **Compare model performance** using various evaluation metrics  \n",
        "3. **Understand algorithm strengths and weaknesses** in practice\n",
        "4. **Make informed decisions** about model selection\n",
        "5. **Create visualizations** to communicate results effectively\n",
        "\n",
        "### Key ML Concepts Practiced\n",
        "\n",
        "- Complete ML Pipeline: From raw data to model evaluation\n",
        "- Data Preprocessing: Handling missing values, encoding, scaling\n",
        "- Model Building: Implementing 9 different algorithms\n",
        "- Model Evaluation: Accuracy, precision, recall, F1-Score, ROC-AUC\n",
        "- Cross-Validation: Ensuring robust model evaluation\n",
        "- Visualization: Creating comparison charts and performance plots\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Overview\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "In this project, you will work with a **Heart Disease Prediction** dataset to:\n",
        "- Compare 9 different ML algorithms\n",
        "- Evaluate their performance comprehensively\n",
        "- Identify the best model for this medical diagnosis problem\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "In real-world ML projects:\n",
        "- **No single algorithm works best for all problems**\n",
        "- **Data characteristics matter**: Dataset size, features, noise, class balance\n",
        "- **Different metrics reveal different insights**: Accuracy alone is not enough\n",
        "- **Computational cost vs. performance**: Sometimes simpler models are better\n",
        "\n",
        "### Algorithms You Will Compare\n",
        "\n",
        "| Algorithm | Type | Best Use Cases |\n",
        "|-----------|------|----------------|\n",
        "| Logistic Regression | Linear | Baseline, interpretability |\n",
        "| Decision Tree | Tree-based | Non-linear patterns, interpretability |\n",
        "| Random Forest | Ensemble | High accuracy, feature importance |\n",
        "| Gradient Boosting | Ensemble | Competition-winning performance |\n",
        "| XGBoost | Ensemble (Advanced) | State-of-the-art performance |\n",
        "| LightGBM | Ensemble (Advanced) | Large datasets, speed |\n",
        "| Support Vector Machine | Kernel-based | High-dimensional data |\n",
        "| k-Nearest Neighbors | Instance-based | Small datasets, non-parametric |\n",
        "| Naive Bayes | Probabilistic | Fast training, works with small data |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Data Loading\n",
        "\n",
        "Let's start by importing all necessary libraries and loading our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import all required libraries\n",
        "# Data manipulation: numpy, pandas\n",
        "# Visualization: matplotlib, seaborn  \n",
        "# Configure warnings and matplotlib inline\n",
        "# Preprocessing: train_test_split, cross_val_score, StratifiedKFold, StandardScaler, SimpleImputer\n",
        "# Models: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier, \n",
        "#         SVC, KNeighborsClassifier, GaussianNB, XGBClassifier, LGBMClassifier\n",
        "# Metrics: accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \n",
        "#          confusion_matrix, classification_report, roc_curve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "**Dataset**: Heart Disease UCI  \n",
        "**URL**: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data  \n",
        "**Target**: Binary classification (0 = No disease, 1 = Disease)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "\n",
        "# TODO: Load dataset with pd.read_csv(url, names=column_names, na_values='?')\n",
        "# TODO: Convert target to binary: (df['target'] > 0).astype(int)\n",
        "# TODO: Display shape and first few rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Alternative you can run below cell and use this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Alternative: Create a dummy dataset for practice\n",
        "# # Use this if you want to practice without downloading from UCI\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Set random seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "# # Generate dummy data with 300 samples\n",
        "# n_samples = 300\n",
        "\n",
        "# # Create dummy features based on real heart disease characteristics\n",
        "# data = {\n",
        "#     'age': np.random.normal(54, 9, n_samples).astype(int),  # Age 45-65\n",
        "#     'sex': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),  # 70% male\n",
        "#     'cp': np.random.choice([0, 1, 2, 3], n_samples, p=[0.4, 0.3, 0.2, 0.1]),  # Chest pain type\n",
        "#     'trestbps': np.random.normal(130, 20, n_samples).astype(int),  # Resting BP\n",
        "#     'chol': np.random.normal(250, 60, n_samples).astype(int),  # Cholesterol\n",
        "#     'fbs': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),  # Fasting blood sugar\n",
        "#     'restecg': np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.3, 0.2]),  # Resting ECG\n",
        "#     'thalach': np.random.normal(150, 25, n_samples).astype(int),  # Max heart rate\n",
        "#     'exang': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Exercise angina\n",
        "#     'oldpeak': np.random.exponential(1, n_samples).round(1),  # ST depression\n",
        "#     'slope': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2]),  # Slope\n",
        "#     'ca': np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05]),  # Vessels\n",
        "#     'thal': np.random.choice([3, 6, 7], n_samples, p=[0.7, 0.2, 0.1]),  # Thalassemia\n",
        "# }\n",
        "\n",
        "# # Create target variable with some logical relationships\n",
        "# target = []\n",
        "# for i in range(n_samples):\n",
        "#     prob = 0.3  # Base probability\n",
        "    \n",
        "#     # Increase probability based on risk factors\n",
        "#     if data['age'][i] > 55: prob += 0.2\n",
        "#     if data['sex'][i] == 1: prob += 0.1  # Male\n",
        "#     if data['cp'][i] in [2, 3]: prob += 0.15  # Atypical chest pain\n",
        "#     if data['trestbps'][i] > 140: prob += 0.1  # High BP\n",
        "#     if data['chol'][i] > 280: prob += 0.1  # High cholesterol\n",
        "#     if data['exang'][i] == 1: prob += 0.2  # Exercise angina\n",
        "#     if data['oldpeak'][i] > 1: prob += 0.15  # ST depression\n",
        "#     if data['ca'][i] > 0: prob += 0.1  # Vessel narrowing\n",
        "    \n",
        "#     # Add some noise\n",
        "#     prob += np.random.normal(0, 0.1)\n",
        "#     prob = max(0, min(1, prob))  # Clamp between 0 and 1\n",
        "    \n",
        "#     target.append(1 if np.random.random() < prob else 0)\n",
        "\n",
        "# data['target'] = target\n",
        "\n",
        "# # Create DataFrame\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# # Add some missing values (about 5% randomly)\n",
        "# missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
        "# missing_cols = ['ca', 'thal', 'oldpeak']  # These often have missing values\n",
        "# for idx in missing_indices:\n",
        "#     col = np.random.choice(missing_cols)\n",
        "#     df.loc[idx, col] = np.nan\n",
        "\n",
        "# print(\"Dummy Dataset Created!\")\n",
        "# print(f\"Shape: {df.shape}\")\n",
        "# print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")\n",
        "# print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "# print(\"\\nFirst few rows:\")\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Display dataset info using df.info()\n",
        "# TODO: Display statistical summary using df.describe()\n",
        "# TODO: Check missing values using df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reflection Question 1:** What do you notice about missing values? How will this affect model building?\n",
        "\n",
        "*Write your observations here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Exploratory Data Analysis\n",
        "\n",
        "Before building models, analyze the data to understand patterns and relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Visualize target distribution with bar chart and pie chart\n",
        "# TODO: Calculate class distribution percentages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Plot distributions of numerical features\n",
        "# Suggested: age, trestbps, chol, thalach, oldpeak\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create correlation heatmap\n",
        "# TODO: Find and display features most correlated with target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reflection Question 2:** Which features are most strongly correlated with heart disease? Any multicollinearity issues?\n",
        "\n",
        "*Write your insights here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Data Preprocessing\n",
        "\n",
        "Prepare the data for modeling by handling missing values, splitting data, and scaling features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Handle missing values using SimpleImputer with median strategy\n",
        "# TODO: Verify no missing values remain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Separate features (X) and target (y)\n",
        "# TODO: Perform stratified train-test split (test_size=0.2, random_state=42)\n",
        "# TODO: Display split sizes and class distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Scale features using StandardScaler\n",
        "# Important: Fit only on training data to prevent data leakage\n",
        "# TODO: Verify scaling (mean ~0, std ~1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reflection Question 3:** Why fit scaler only on training data? Explain data leakage.\n",
        "\n",
        "*Explain the concept:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Model Training and Comparison\n",
        "\n",
        "Train multiple algorithms and collect their predictions for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create models dictionary with 9 algorithms:\n",
        "# - Logistic Regression, Decision Tree, Random Forest\n",
        "# - Gradient Boosting, XGBoost, LightGBM\n",
        "# - SVM, k-Nearest Neighbors, Naive Bayes\n",
        "# TODO: Print the number of models and their names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Loop through models and:\n",
        "# 1. Train on X_train_scaled, y_train\n",
        "# 2. Get predictions on X_test_scaled\n",
        "# 3. Get probability predictions (if available)\n",
        "# 4. Calculate: accuracy, precision, recall, f1_score, ROC-AUC\n",
        "# 5. Store results in dictionaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Model Evaluation and Visualization\n",
        "\n",
        "Analyze and visualize model performance using multiple metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create results DataFrame and sort by accuracy\n",
        "# TODO: Display formatted comparison table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create horizontal bar chart comparing model accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create grouped bar chart for multiple metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Plot ROC curves for all models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create confusion matrices for top 3 models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Print classification reports for top 3 models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reflection Question 4:** Which model performed best? For medical diagnosis, which metric matters most and why?\n",
        "\n",
        "*Write your analysis here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Cross-Validation Analysis\n",
        "\n",
        "Use cross-validation to get more robust performance estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Perform 5-fold stratified cross-validation for all models\n",
        "# TODO: Store mean, std, and test accuracy\n",
        "# TODO: Display CV results table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create error bar plot for CV results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reflection Question 5:** How do CV results compare to test results? Which model shows most stability?\n",
        "\n",
        "*Write your analysis here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7: Final Model Selection and Insights\n",
        "\n",
        "Synthesize all metrics to determine the best overall model and provide actionable insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Combine all metrics and calculate overall ranking\n",
        "# TODO: Display comprehensive summary of top 3 models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final Reflection:** Which model would you deploy and why? Consider performance, interpretability, and deployment constraints.\n",
        "\n",
        "*Write your decision and justification here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Insights and Recommendations\n",
        "\n",
        "### Algorithm Characteristics\n",
        "\n",
        "**Ensemble Methods** - Random Forest, XGBoost, LightGBM typically perform best due to handling non-linear relationships well.\n",
        "\n",
        "**Linear Models** - Logistic Regression provides interpretable baseline with fast training.\n",
        "\n",
        "**Kernel Methods** - SVM sensitive to scaling; computationally expensive for large datasets.\n",
        "\n",
        "**Instance-Based** - kNN depends heavily on parameter k and distance metric.\n",
        "\n",
        "### Recommendations by Use Case\n",
        "\n",
        "**Production Deployment:** Choose top ensemble method for best accuracy and robustness.\n",
        "\n",
        "**Interpretability:** Choose Logistic Regression or Decision Tree when transparency is critical.\n",
        "\n",
        "**Real-Time:** Choose LightGBM or Logistic Regression for fastest inference.\n",
        "\n",
        "**Medical Context:** Recall is crucial - minimize false negatives.\n",
        "\n",
        "---\n",
        "\n",
        "## Optional Challenges (Bonus)\n",
        "\n",
        "1. **Hyperparameter Tuning**: Use GridSearchCV to optimize your top model\n",
        "2. **Feature Selection**: Implement and compare performance with selected features  \n",
        "3. **Ensemble Creation**: Build voting/stacking classifier with top 3 models\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Submission Guidelines\n",
        "\n",
        "### Submission Requirements\n",
        "\n",
        "Before submitting your project, ensure you have:\n",
        "\n",
        "1. **Completed all TODO sections** with working code\n",
        "2. **Answered all reflection questions** with detailed analysis\n",
        "3. **Generated all required visualizations** \n",
        "4. **Tested code and verified** all outputs\n",
        "5. **Documented findings** and insights\n",
        "\n",
        "### How to Submit\n",
        "\n",
        "**Step 1:** Send LinkedIn connection request to: https://www.linkedin.com/in/hashirahmed07/\n",
        "\n",
        "**Step 2:** Clean up notebook and ensure all cells run without errors\n",
        "\n",
        "**Step 3:** Submit with title format: **30_Days_ML_Practice_Project_Week2**\n",
        "\n",
        "### Evaluation Criteria\n",
        "\n",
        "Your project will be evaluated based on:\n",
        "\n",
        "- **Code Quality (30%)** - Correctness, organization, readability\n",
        "- **Analysis Quality (30%)** - Depth of EDA and interpretation  \n",
        "- **Visualization (20%)** - Clarity and professionalism\n",
        "- **Insights (20%)** - Model comparison and justification\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your project! Remember: The goal is to learn and practice, not just to get perfect results.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ðŸ“« Let's Connect\n",
        "- ðŸ’¼ **LinkedIn:** [hashirahmed07](https://www.linkedin.com/in/hashirahmed07/)\n",
        "- ðŸ“§ **Email:** [Hashirahmad330@gmail.com](mailto:Hashirahmad330@gmail.com)\n",
        "- ðŸ™ **GitHub:** [CodeByHashir](https://github.com/CodeByHashir)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
