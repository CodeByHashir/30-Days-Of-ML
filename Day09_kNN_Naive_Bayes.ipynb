{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 9: k-Nearest Neighbors (kNN) & Naive Bayes - Lazy Learning & Probabilistic Classification\n",
        "\n",
        "**Welcome to Day 9 of your ML journey!** Today we'll explore two fascinating algorithms that approach machine learning from completely different angles: **k-Nearest Neighbors (kNN)** - the \"lazy learner\" that makes decisions based on similarity, and **Naive Bayes** - the probabilistic classifier that uses statistical reasoning to make predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Master kNN and Naive Bayes with focus on when and why they work, plus their unique advantages in real-world scenarios.\n",
        "\n",
        "**Topics Covered:**\n",
        "- kNN intuition and the concept of \"lazy learning\"\n",
        "- Distance metrics and similarity measures\n",
        "- Naive Bayes probability theory and independence assumption\n",
        "- When to use each algorithm\n",
        "- Hyperparameter tuning and optimization\n",
        "- Real-world applications and performance comparison\n",
        "- Advantages and limitations of both approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Concept Overview\n",
        "\n",
        "### What is k-Nearest Neighbors (kNN)?\n",
        "\n",
        "**k-Nearest Neighbors (kNN)** is a simple yet powerful algorithm that classifies data points based on the majority class of their k nearest neighbors. It's called a \"lazy learner\" because it doesn't build a model during training - it simply stores the training data and makes predictions when needed.\n",
        "\n",
        "**The Core Intuition:**\n",
        "Think of kNN like asking your neighbors for advice. When you need to make a decision, you ask the k closest people around you, and you go with what the majority of them recommend.\n",
        "\n",
        "**Real-World Example:**\n",
        "- **Movie Recommendations:** \"People who liked these movies also liked...\"\n",
        "- **Medical Diagnosis:** \"Patients with similar symptoms had this condition\"\n",
        "- **Real Estate:** \"Houses in similar neighborhoods sold for this price\"\n",
        "- **Customer Segmentation:** \"Customers with similar purchase history belong to this segment\"\n",
        "\n",
        "**Why kNN is Powerful:**\n",
        "1. **No assumptions about data distribution**\n",
        "2. **Works well with non-linear patterns**\n",
        "3. **Simple to understand and implement**\n",
        "4. **Handles both classification and regression**\n",
        "5. **No training phase - just store the data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance Metrics in kNN\n",
        "\n",
        "**The Key Question:** How do we measure \"closeness\" between data points?\n",
        "\n",
        "**Common Distance Metrics:**\n",
        "\n",
        "1. **Euclidean Distance** (Most Common):\n",
        "   ```\n",
        "   d = √[(x₁-x₂)² + (y₁-y₂)² + (z₁-z₂)² + ...]\n",
        "   ```\n",
        "   - Straight-line distance between two points\n",
        "   - Works best when all features have similar scales\n",
        "\n",
        "2. **Manhattan Distance**:\n",
        "   ```\n",
        "   d = |x₁-x₂| + |y₁-y₂| + |z₁-z₂| + ...\n",
        "   ```\n",
        "   - Sum of absolute differences\n",
        "   - Less sensitive to outliers than Euclidean\n",
        "\n",
        "3. **Minkowski Distance** (Generalized):\n",
        "   ```\n",
        "   d = (|x₁-x₂|ᵖ + |y₁-y₂|ᵖ + |z₁-z₂|ᵖ)^(1/p)\n",
        "   ```\n",
        "   - Euclidean when p=2, Manhattan when p=1\n",
        "\n",
        "**Feature Scaling is Critical:**\n",
        "- Features with larger scales dominate distance calculations\n",
        "- Always normalize or standardize features before using kNN\n",
        "- Without scaling, a feature measured in thousands will overshadow one measured in decimals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Naive Bayes?\n",
        "\n",
        "**Naive Bayes** is a probabilistic classifier based on Bayes' theorem with a strong independence assumption between features. Despite this \"naive\" assumption, it often works remarkably well in practice.\n",
        "\n",
        "**The Core Intuition:**\n",
        "Think of Naive Bayes like a detective using evidence to solve a case. Each piece of evidence (feature) contributes to the probability of different outcomes (classes), and the detective combines all evidence to make the final decision.\n",
        "\n",
        "**Real-World Example:**\n",
        "- **Email Spam Detection:** \"Words like 'free', 'money', 'urgent' increase spam probability\"\n",
        "- **Medical Diagnosis:** \"Symptoms like fever + cough + headache suggest flu\"\n",
        "- **Sentiment Analysis:** \"Words like 'great', 'amazing', 'love' indicate positive sentiment\"\n",
        "- **Customer Churn:** \"Low usage + support tickets + payment delays predict churn\"\n",
        "\n",
        "**Why Naive Bayes is Powerful:**\n",
        "1. **Fast training and prediction**\n",
        "2. **Works well with high-dimensional data**\n",
        "3. **Handles missing data gracefully**\n",
        "4. **Provides probability estimates**\n",
        "5. **Simple to implement and understand**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes Mathematics - The \"Naive\" Assumption\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "```\n",
        "P(Class|Features) = P(Features|Class) × P(Class) / P(Features)\n",
        "```\n",
        "\n",
        "**The \"Naive\" Independence Assumption:**\n",
        "Naive Bayes assumes that all features are independent of each other given the class. This means:\n",
        "```\n",
        "P(Feature₁, Feature₂, ..., Featureₙ|Class) = P(Feature₁|Class) × P(Feature₂|Class) × ... × P(Featureₙ|Class)\n",
        "```\n",
        "\n",
        "**Why This Works Despite Being \"Naive\":**\n",
        "1. **Real-world features often aren't perfectly independent**, but Naive Bayes is surprisingly robust\n",
        "2. **The independence assumption simplifies calculations** dramatically\n",
        "3. **In many cases, the algorithm gets the relative ordering of probabilities correct**\n",
        "4. **It's computationally efficient** and scales well to high dimensions\n",
        "\n",
        "**Types of Naive Bayes:**\n",
        "1. **Gaussian Naive Bayes**: Assumes features follow normal distribution\n",
        "2. **Multinomial Naive Bayes**: For discrete features (like word counts)\n",
        "3. **Bernoulli Naive Bayes**: For binary features (like presence/absence of words)\n",
        "\n",
        "### When to Use Each Algorithm\n",
        "\n",
        "**Use kNN When:**\n",
        "- You have a small to medium dataset\n",
        "- Data has complex, non-linear patterns\n",
        "- You need interpretable results\n",
        "- Features are already normalized\n",
        "- You can afford slower prediction times\n",
        "\n",
        "**Use Naive Bayes When:**\n",
        "- You have high-dimensional data (many features)\n",
        "- You need fast training and prediction\n",
        "- You want probability estimates\n",
        "- You have limited computational resources\n",
        "- Features are roughly independent\n",
        "\n",
        "**Avoid kNN When:**\n",
        "- Dataset is very large (millions of samples)\n",
        "- You need fast prediction times\n",
        "- Features have very different scales\n",
        "- Data has many irrelevant features\n",
        "\n",
        "**Avoid Naive Bayes When:**\n",
        "- Features are highly correlated\n",
        "- You need the most accurate model possible\n",
        "- You have very few training examples per class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Code Demo\n",
        "\n",
        "Let's explore kNN and Naive Bayes with practical examples using different datasets and scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.datasets import make_classification, load_iris, load_wine\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo 1: kNN Classification - The Effect of k\n",
        "\n",
        "Let's start with a simple classification problem to understand how the choice of k affects kNN performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features (crucial for kNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different values of k\n",
        "k_values = [1, 3, 5, 7, 9, 15, 21, 31]\n",
        "accuracies = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"k={k:2d}: Accuracy = {accuracy:.3f}\")\n",
        "\n",
        "# Find the best k\n",
        "best_k = k_values[np.argmax(accuracies)]\n",
        "print(f\"\\nBest k value: {best_k} with accuracy: {max(accuracies):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the effect of different k values\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Accuracy vs k\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
        "plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Best k = {best_k}')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('kNN Accuracy vs k Value')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Plot 2: Decision boundaries for different k values\n",
        "plt.subplot(1, 2, 2)\n",
        "selected_k_values = [1, 3, 9, 21]\n",
        "colors = ['red', 'blue', 'green', 'orange']\n",
        "\n",
        "for i, k in enumerate(selected_k_values):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Create a mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "    y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Make predictions on the mesh\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot the contour\n",
        "    plt.contour(xx, yy, Z, colors=colors[i], alpha=0.6, linewidths=2)\n",
        "\n",
        "# Plot the training data\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
        "           cmap='RdYlBu', edgecolors='black', alpha=0.8)\n",
        "plt.xlabel('Feature 1 (scaled)')\n",
        "plt.ylabel('Feature 2 (scaled)')\n",
        "plt.title('Decision Boundaries for Different k Values')\n",
        "plt.legend([f'k={k}' for k in selected_k_values])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Observations:**\n",
        "- **k=1**: Creates very jagged, complex boundaries (overfitting) - follows every training point exactly\n",
        "- **k=3**: Still complex but smoother - good balance for this dataset\n",
        "- **k=9**: Smooth, generalizable boundary - often optimal for many problems\n",
        "- **k=21**: Very smooth but may underfit - loses important details\n",
        "\n",
        "**The Bias-Variance Tradeoff in kNN:**\n",
        "- **Small k (low bias, high variance)**: Model follows training data closely, prone to overfitting\n",
        "- **Large k (high bias, low variance)**: Model is smoother but may miss important patterns\n",
        "\n",
        "**Rule of Thumb:** Start with k = √n (where n is the number of training samples) and tune from there.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo 2: Naive Bayes - Different Types and Applications\n",
        "\n",
        "Let's explore the different types of Naive Bayes classifiers and see how they perform on different types of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "# Split the data\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris)\n",
        "\n",
        "# Test different Naive Bayes classifiers\n",
        "nb_classifiers = {\n",
        "    'Gaussian NB': GaussianNB(),\n",
        "    'Multinomial NB': MultinomialNB(),\n",
        "    'Bernoulli NB': BernoulliNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nNaive Bayes Performance Comparison:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for name, classifier in nb_classifiers.items():\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_iris, y_train_iris)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = classifier.predict(X_test_iris)\n",
        "    accuracy = accuracy_score(y_test_iris, y_pred)\n",
        "    \n",
        "    results[name] = {\n",
        "        'classifier': classifier,\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"{name:15s}: {accuracy:.3f}\")\n",
        "\n",
        "# Find the best performing classifier\n",
        "best_nb = max(results, key=lambda x: results[x]['accuracy'])\n",
        "print(f\"\\nBest Naive Bayes: {best_nb} with accuracy: {results[best_nb]['accuracy']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report for the best Naive Bayes\n",
        "print(f\"\\nDetailed Classification Report - {best_nb}:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test_iris, results[best_nb]['predictions'], \n",
        "                          target_names=class_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_iris, results[best_nb]['predictions'])\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(f'Confusion Matrix - {best_nb}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance analysis using Naive Bayes\n",
        "print(\"\\nFeature Importance Analysis:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Get feature importance from Gaussian NB (it has theta_ and var_ attributes)\n",
        "if hasattr(results['Gaussian NB']['classifier'], 'theta_'):\n",
        "    gaussian_nb = results['Gaussian NB']['classifier']\n",
        "    \n",
        "    # Calculate feature importance based on class separation\n",
        "    feature_importance = np.zeros(len(feature_names))\n",
        "    \n",
        "    for class_idx in range(len(class_names)):\n",
        "        for feature_idx in range(len(feature_names)):\n",
        "            # Calculate how much this feature varies across classes\n",
        "            class_mean = gaussian_nb.theta_[class_idx, feature_idx]\n",
        "            class_var = gaussian_nb.var_[class_idx, feature_idx]\n",
        "            \n",
        "            # Features with lower variance and distinct means are more important\n",
        "            feature_importance[feature_idx] += 1 / (class_var + 1e-8)\n",
        "    \n",
        "    # Normalize importance scores\n",
        "    feature_importance = feature_importance / np.sum(feature_importance)\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(feature_names, feature_importance, color='skyblue', alpha=0.7)\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Relative Importance')\n",
        "    plt.title('Feature Importance from Gaussian Naive Bayes')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, importance in zip(bars, feature_importance):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                f'{importance:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understanding the Results:**\n",
        "\n",
        "**Why Gaussian NB Performed Best:**\n",
        "- **Iris features are continuous** (measurements like sepal length, width)\n",
        "- **Gaussian NB assumes normal distribution** - appropriate for continuous data\n",
        "- **Multinomial NB** expects discrete counts (like word frequencies)\n",
        "- **Bernoulli NB** expects binary features (present/absent)\n",
        "\n",
        "**Feature Importance Insights:**\n",
        "- **Sepal length and width** are often the most discriminative features\n",
        "- **Petal measurements** also contribute significantly to classification\n",
        "- This aligns with botanical knowledge about iris species differences\n",
        "\n",
        "**Key Takeaway:** Choose the right Naive Bayes variant based on your data type:\n",
        "- **Gaussian**: Continuous features (measurements, ratings)\n",
        "- **Multinomial**: Count data (word frequencies, purchase counts)\n",
        "- **Bernoulli**: Binary features (presence/absence of words, yes/no responses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo 3: kNN vs Naive Bayes - Head-to-Head Comparison\n",
        "\n",
        "Let's compare kNN and Naive Bayes on the same dataset to understand their relative strengths and weaknesses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Wine dataset for a more challenging comparison\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "wine_feature_names = wine.feature_names\n",
        "wine_class_names = wine.target_names\n",
        "\n",
        "print(f\"Wine dataset shape: {X_wine.shape}\")\n",
        "print(f\"Number of features: {len(wine_feature_names)}\")\n",
        "print(f\"Classes: {wine_class_names}\")\n",
        "\n",
        "# Split the data\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine)\n",
        "\n",
        "# Scale features for kNN\n",
        "scaler_wine = StandardScaler()\n",
        "X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)\n",
        "X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n",
        "\n",
        "# Train both models\n",
        "import time\n",
        "\n",
        "# kNN with optimal k (found through cross-validation)\n",
        "knn_wine = KNeighborsClassifier(n_neighbors=5)\n",
        "start_time = time.time()\n",
        "knn_wine.fit(X_train_wine_scaled, y_train_wine)\n",
        "knn_training_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "knn_pred = knn_wine.predict(X_test_wine_scaled)\n",
        "knn_prediction_time = time.time() - start_time\n",
        "\n",
        "# Naive Bayes\n",
        "nb_wine = GaussianNB()\n",
        "start_time = time.time()\n",
        "nb_wine.fit(X_train_wine, y_train_wine)  # No scaling needed for Gaussian NB\n",
        "nb_training_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nb_pred = nb_wine.predict(X_test_wine)\n",
        "nb_prediction_time = time.time() - start_time\n",
        "\n",
        "# Calculate accuracies\n",
        "knn_accuracy = accuracy_score(y_test_wine, knn_pred)\n",
        "nb_accuracy = accuracy_score(y_test_wine, nb_pred)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"\\nAlgorithm Comparison Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Metric':<20} {'kNN':<10} {'Naive Bayes':<15}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Accuracy':<20} {knn_accuracy:<10.3f} {nb_accuracy:<15.3f}\")\n",
        "print(f\"{'Training Time (s)':<20} {knn_training_time:<10.4f} {nb_training_time:<15.4f}\")\n",
        "print(f\"{'Prediction Time (s)':<20} {knn_prediction_time:<10.4f} {nb_prediction_time:<15.4f}\")\n",
        "\n",
        "# Cross-validation comparison\n",
        "print(\"\\nCross-Validation Results (5-fold):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# kNN cross-validation\n",
        "knn_scores = cross_val_score(knn_wine, X_train_wine_scaled, y_train_wine, cv=5)\n",
        "print(f\"kNN CV Scores: {knn_scores}\")\n",
        "print(f\"kNN CV Mean: {knn_scores.mean():.3f} (+/- {knn_scores.std() * 2:.3f})\")\n",
        "\n",
        "# Naive Bayes cross-validation\n",
        "nb_scores = cross_val_score(nb_wine, X_train_wine, y_train_wine, cv=5)\n",
        "print(f\"Naive Bayes CV Scores: {nb_scores}\")\n",
        "print(f\"Naive Bayes CV Mean: {nb_scores.mean():.3f} (+/- {nb_scores.std() * 2:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Accuracy comparison\n",
        "axes[0, 0].bar(['kNN', 'Naive Bayes'], [knn_accuracy, nb_accuracy], \n",
        "               color=['skyblue', 'lightcoral'], alpha=0.7)\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Comparison')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "for i, v in enumerate([knn_accuracy, nb_accuracy]):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Training time comparison\n",
        "axes[0, 1].bar(['kNN', 'Naive Bayes'], [knn_training_time, nb_training_time], \n",
        "               color=['lightgreen', 'orange'], alpha=0.7)\n",
        "axes[0, 1].set_ylabel('Training Time (seconds)')\n",
        "axes[0, 1].set_title('Training Time Comparison')\n",
        "for i, v in enumerate([knn_training_time, nb_training_time]):\n",
        "    axes[0, 1].text(i, v + 0.0001, f'{v:.4f}s', ha='center', va='bottom')\n",
        "\n",
        "# Plot 3: Prediction time comparison\n",
        "axes[1, 0].bar(['kNN', 'Naive Bayes'], [knn_prediction_time, nb_prediction_time], \n",
        "               color=['purple', 'brown'], alpha=0.7)\n",
        "axes[1, 0].set_ylabel('Prediction Time (seconds)')\n",
        "axes[1, 0].set_title('Prediction Time Comparison')\n",
        "for i, v in enumerate([knn_prediction_time, nb_prediction_time]):\n",
        "    axes[1, 0].text(i, v + 0.0001, f'{v:.4f}s', ha='center', va='bottom')\n",
        "\n",
        "# Plot 4: Cross-validation scores comparison\n",
        "x_pos = np.arange(2)\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x_pos - width/2, [knn_scores.mean(), nb_scores.mean()], width, \n",
        "               yerr=[knn_scores.std(), nb_scores.std()], \n",
        "               color=['skyblue', 'lightcoral'], alpha=0.7, \n",
        "               capsize=5, label='Mean CV Score')\n",
        "axes[1, 1].set_ylabel('Cross-Validation Score')\n",
        "axes[1, 1].set_title('Cross-Validation Comparison')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(['kNN', 'Naive Bayes'])\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nDetailed Classification Reports:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nkNN Classification Report:\")\n",
        "print(\"-\" * 30)\n",
        "print(classification_report(y_test_wine, knn_pred, target_names=wine_class_names))\n",
        "\n",
        "print(\"\\nNaive Bayes Classification Report:\")\n",
        "print(\"-\" * 30)\n",
        "print(classification_report(y_test_wine, nb_pred, target_names=wine_class_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Insights from the Comparison:**\n",
        "\n",
        "**Performance Trade-offs:**\n",
        "- **kNN**: Often higher accuracy, especially with proper tuning and feature scaling\n",
        "- **Naive Bayes**: Generally faster training and prediction, but may sacrifice some accuracy\n",
        "\n",
        "**Speed Considerations:**\n",
        "- **Training Time**: Naive Bayes is typically much faster (no model building required)\n",
        "- **Prediction Time**: Naive Bayes is faster (simple probability calculations vs distance computations)\n",
        "\n",
        "**When to Choose Each:**\n",
        "\n",
        "**Choose kNN When:**\n",
        "- You need the highest possible accuracy\n",
        "- You have time to tune hyperparameters\n",
        "- Your dataset is small to medium-sized\n",
        "- Features are properly scaled\n",
        "- You can afford slower prediction times\n",
        "\n",
        "**Choose Naive Bayes When:**\n",
        "- You need fast training and prediction\n",
        "- You have high-dimensional data\n",
        "- You want probability estimates\n",
        "- You have limited computational resources\n",
        "- Features are roughly independent\n",
        "\n",
        "**Real-World Decision Framework:**\n",
        "1. **Start with Naive Bayes** for quick baseline results\n",
        "2. **Use kNN** if you need higher accuracy and can afford the computational cost\n",
        "3. **Consider ensemble methods** combining both for optimal performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo 4: Hyperparameter Tuning - Finding the Optimal k\n",
        "\n",
        "Let's use GridSearchCV to systematically find the best k value for kNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more complex dataset for tuning demonstration\n",
        "X_complex, y_complex = make_classification(n_samples=500, n_features=4, n_redundant=0, \n",
        "                                          n_informative=4, n_clusters_per_class=2, \n",
        "                                          random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train_complex, X_test_complex, y_train_complex, y_test_complex = train_test_split(\n",
        "    X_complex, y_complex, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler_complex = StandardScaler()\n",
        "X_train_complex_scaled = scaler_complex.fit_transform(X_train_complex)\n",
        "X_test_complex_scaled = scaler_complex.transform(X_test_complex)\n",
        "\n",
        "# Define parameter grid for kNN\n",
        "param_grid = {\n",
        "    'n_neighbors': range(1, 31, 2),  # Test odd values from 1 to 29\n",
        "    'weights': ['uniform', 'distance'],  # Uniform vs distance-weighted voting\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']  # Different distance metrics\n",
        "}\n",
        "\n",
        "print(\"Parameter Grid for kNN Tuning:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"n_neighbors: {list(param_grid['n_neighbors'])}\")\n",
        "print(f\"weights: {param_grid['weights']}\")\n",
        "print(f\"metric: {param_grid['metric']}\")\n",
        "print(f\"Total combinations: {len(param_grid['n_neighbors']) * len(param_grid['weights']) * len(param_grid['metric'])}\")\n",
        "\n",
        "# Grid search with cross-validation\n",
        "knn_grid = GridSearchCV(\n",
        "    KNeighborsClassifier(), \n",
        "    param_grid, \n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "print(\"\\nStarting Grid Search...\")\n",
        "knn_grid.fit(X_train_complex_scaled, y_train_complex)\n",
        "\n",
        "# Results\n",
        "print(f\"\\nBest parameters: {knn_grid.best_params_}\")\n",
        "print(f\"Best cross-validation score: {knn_grid.best_score_:.3f}\")\n",
        "\n",
        "# Test on unseen data\n",
        "best_knn = knn_grid.best_estimator_\n",
        "y_pred_complex = best_knn.predict(X_test_complex_scaled)\n",
        "test_accuracy = accuracy_score(y_test_complex, y_pred_complex)\n",
        "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
        "\n",
        "# Analyze the results\n",
        "results_df = pd.DataFrame(knn_grid.cv_results_)\n",
        "print(f\"\\nTop 5 parameter combinations:\")\n",
        "print(\"-\" * 50)\n",
        "top_5 = results_df.nlargest(5, 'mean_test_score')[['param_n_neighbors', 'param_weights', 'param_metric', 'mean_test_score']]\n",
        "print(top_5.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understanding kNN Hyperparameters:**\n",
        "\n",
        "**n_neighbors (k value):**\n",
        "- **Small k**: More sensitive to noise, prone to overfitting\n",
        "- **Large k**: Smoother decision boundaries, but may underfit\n",
        "- **Rule of thumb**: Start with k = √n, then tune\n",
        "\n",
        "**weights:**\n",
        "- **'uniform'**: All k neighbors have equal vote\n",
        "- **'distance'**: Closer neighbors have more influence\n",
        "- **Distance weighting** often improves performance\n",
        "\n",
        "**metric:**\n",
        "- **'euclidean'**: Standard straight-line distance\n",
        "- **'manhattan'**: Sum of absolute differences (less sensitive to outliers)\n",
        "- **'minkowski'**: Generalized distance (euclidean when p=2)\n",
        "\n",
        "**Grid Search Benefits:**\n",
        "- **Systematic exploration** of all parameter combinations\n",
        "- **Cross-validation** prevents overfitting to validation set\n",
        "- **Reproducible results** with proper random seeds\n",
        "- **Comprehensive evaluation** of hyperparameter space\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Hands-on Exercise\n",
        "\n",
        "Now it's your turn! Complete these exercises to solidify your understanding of kNN and Naive Bayes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: kNN Hyperparameter Investigation\n",
        "\n",
        "**Task:** Investigate how different hyperparameters affect kNN performance on the digits dataset.\n",
        "\n",
        "**Instructions:**\n",
        "1. Load the digits dataset from sklearn\n",
        "2. Split the data and scale features appropriately\n",
        "3. Test different values of k (1, 3, 5, 7, 9, 11, 15, 21)\n",
        "4. Compare uniform vs distance-weighted voting\n",
        "5. Visualize the results and explain which hyperparameters work best\n",
        "\n",
        "**Starter Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# from sklearn.datasets import load_digits\n",
        "# \n",
        "# # Load digits dataset\n",
        "# digits = load_digits()\n",
        "# X_digits = digits.data\n",
        "# y_digits = digits.target\n",
        "# \n",
        "# # Split and scale data\n",
        "# X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(\n",
        "#     X_digits, y_digits, test_size=0.3, random_state=42)\n",
        "# \n",
        "# # Scale features (crucial for kNN)\n",
        "# scaler_digits = StandardScaler()\n",
        "# X_train_digits_scaled = scaler_digits.fit_transform(X_train_digits)\n",
        "# X_test_digits_scaled = scaler_digits.transform(X_test_digits)\n",
        "# \n",
        "# # Test different k values and weights\n",
        "# k_values = [1, 3, 5, 7, 9, 11, 15, 21]\n",
        "# weights_options = ['uniform', 'distance']\n",
        "# \n",
        "# # Complete the implementation...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Naive Bayes Text Classification Simulation\n",
        "\n",
        "**Task:** Simulate text classification using Naive Bayes with synthetic text-like data.\n",
        "\n",
        "**Instructions:**\n",
        "1. Create a synthetic dataset with binary features representing word presence\n",
        "2. Use Bernoulli Naive Bayes for binary features\n",
        "3. Compare with Gaussian Naive Bayes on the same data\n",
        "4. Analyze feature importance and class probabilities\n",
        "5. Explain why Bernoulli NB might be better for this type of data\n",
        "\n",
        "**Starter Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# import numpy as np\n",
        "# from sklearn.datasets import make_classification\n",
        "# \n",
        "# # Create synthetic binary dataset (simulating word presence/absence)\n",
        "# # This simulates a text classification problem where features are binary\n",
        "# X_binary, y_binary = make_classification(\n",
        "#     n_samples=1000, \n",
        "#     n_features=20,  # 20 \"words\"\n",
        "#     n_redundant=0, \n",
        "#     n_informative=15,  # 15 informative \"words\"\n",
        "#     n_clusters_per_class=1,\n",
        "#     random_state=42\n",
        "# )\n",
        "# \n",
        "# # Convert to binary features (0 or 1) - simulating word presence\n",
        "# X_binary = (X_binary > 0).astype(int)\n",
        "# \n",
        "# # Split the data\n",
        "# X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
        "#     X_binary, y_binary, test_size=0.3, random_state=42)\n",
        "# \n",
        "# # Complete the implementation...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Algorithm Comparison Challenge\n",
        "\n",
        "**Task:** Compare kNN, Naive Bayes, and one additional algorithm on a real-world dataset.\n",
        "\n",
        "**Instructions:**\n",
        "1. Load the breast cancer dataset from sklearn\n",
        "2. Implement kNN, Gaussian Naive Bayes, and Logistic Regression\n",
        "3. Use proper preprocessing (scaling for kNN, no scaling for others)\n",
        "4. Compare accuracy, training time, and prediction time\n",
        "5. Analyze which algorithm works best for this medical dataset and why\n",
        "\n",
        "**Starter Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import time\n",
        "# \n",
        "# # Load breast cancer dataset\n",
        "# cancer = load_breast_cancer()\n",
        "# X_cancer = cancer.data\n",
        "# y_cancer = cancer.target\n",
        "# \n",
        "# # Split the data\n",
        "# X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
        "#     X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer)\n",
        "# \n",
        "# # Complete the implementation...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Takeaways & Next Steps\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "**What You've Learned:**\n",
        "\n",
        "**k-Nearest Neighbors (kNN):**\n",
        "1. **Lazy Learning**: No training phase - just stores data and makes predictions on-demand\n",
        "2. **Distance-Based**: Uses similarity measures to find the k closest neighbors\n",
        "3. **Hyperparameter Sensitivity**: Choice of k, distance metric, and weights significantly affects performance\n",
        "4. **Feature Scaling Critical**: All features must be on similar scales for fair distance calculations\n",
        "5. **Computational Cost**: Training is fast, but prediction can be slow for large datasets\n",
        "\n",
        "**Naive Bayes:**\n",
        "1. **Probabilistic Framework**: Uses Bayes' theorem with independence assumption\n",
        "2. **Fast Training**: No complex optimization - just calculates probability distributions\n",
        "3. **Multiple Variants**: Gaussian, Multinomial, and Bernoulli for different data types\n",
        "4. **Robust Despite \"Naive\"**: Independence assumption often works well in practice\n",
        "5. **Probability Outputs**: Provides class probabilities, not just predictions\n",
        "\n",
        "**When to Use Each Algorithm:**\n",
        "\n",
        "**Choose kNN When:**\n",
        "- You need high accuracy and can afford computational cost\n",
        "- Data has complex, non-linear patterns\n",
        "- You have small to medium datasets\n",
        "- Features are properly scaled\n",
        "- Interpretability is important\n",
        "\n",
        "**Choose Naive Bayes When:**\n",
        "- You need fast training and prediction\n",
        "- You have high-dimensional data\n",
        "- You want probability estimates\n",
        "- Features are roughly independent\n",
        "- You have limited computational resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm Limitations & Considerations\n",
        "\n",
        "**kNN Limitations:**\n",
        "- **Slow prediction** on large datasets (computes distances to all training points)\n",
        "- **Memory intensive** (stores entire training dataset)\n",
        "- **Sensitive to irrelevant features** (distance calculations include all features)\n",
        "- **Curse of dimensionality** (performance degrades with many features)\n",
        "- **No feature importance** (doesn't learn which features matter most)\n",
        "\n",
        "**Naive Bayes Limitations:**\n",
        "- **Independence assumption** may not hold in real-world data\n",
        "- **Can be overconfident** in probability estimates\n",
        "- **Sensitive to feature correlation** (violates independence assumption)\n",
        "- **May perform poorly** with very few training examples per class\n",
        "- **Limited to categorical/continuous features** (requires specific variants)\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "**For kNN:**\n",
        "1. **Always scale features** before training\n",
        "2. **Use cross-validation** to find optimal k\n",
        "3. **Consider distance weighting** for better performance\n",
        "4. **Remove irrelevant features** to improve speed and accuracy\n",
        "5. **Use approximate nearest neighbors** for large datasets\n",
        "\n",
        "**For Naive Bayes:**\n",
        "1. **Choose the right variant** based on your data type\n",
        "2. **Handle missing values** appropriately\n",
        "3. **Consider feature selection** to remove correlated features\n",
        "4. **Use smoothing** (alpha parameter) for small datasets\n",
        "5. **Validate independence assumption** when possible\n",
        "\n",
        "### Next Steps in Your ML Journey\n",
        "\n",
        "**Tomorrow (Day 10):** We'll cover Ensemble Methods - combining multiple algorithms to create more powerful models. This includes Random Forests, Gradient Boosting, and Voting Classifiers.\n",
        "\n",
        "**Further Learning Resources:**\n",
        "\n",
        "**Books:**\n",
        "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
        "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
        "- \"Machine Learning: A Probabilistic Perspective\" by Kevin Murphy\n",
        "\n",
        "**Online Courses:**\n",
        "- Coursera: Machine Learning by Andrew Ng\n",
        "- edX: MIT Introduction to Machine Learning\n",
        "- Fast.ai: Practical Deep Learning for Coders\n",
        "\n",
        "**Practice Datasets:**\n",
        "- Kaggle: kNN and Naive Bayes competitions\n",
        "- UCI Machine Learning Repository\n",
        "- Scikit-learn built-in datasets\n",
        "- Text classification datasets for Naive Bayes practice\n",
        "\n",
        "**Advanced Topics to Explore:**\n",
        "- Locality Sensitive Hashing for approximate kNN\n",
        "- Custom distance metrics for kNN\n",
        "- Laplace smoothing in Naive Bayes\n",
        "- Bayesian Networks (relaxing independence assumption)\n",
        "- kNN for regression problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Recommendations\n",
        "\n",
        "1. **Implement kNN from scratch** using only NumPy to understand the distance calculations\n",
        "2. **Try different distance metrics** on the same dataset and compare results\n",
        "3. **Experiment with feature scaling** - see how performance changes without scaling\n",
        "4. **Compare Naive Bayes variants** on different types of data (continuous, binary, count)\n",
        "5. **Practice on real datasets** from Kaggle or UCI repository\n",
        "6. **Implement text classification** using Naive Bayes on actual text data\n",
        "7. **Study the curse of dimensionality** by gradually adding irrelevant features to kNN\n",
        "\n",
        "### Interview Preparation Tips\n",
        "\n",
        "**Common kNN Interview Questions:**\n",
        "- \"Explain the bias-variance tradeoff in kNN\"\n",
        "- \"How would you handle the curse of dimensionality in kNN?\"\n",
        "- \"What's the time complexity of kNN prediction?\"\n",
        "- \"How do you choose the optimal k value?\"\n",
        "\n",
        "**Common Naive Bayes Interview Questions:**\n",
        "- \"Why is the independence assumption 'naive' and why does it still work?\"\n",
        "- \"Explain the difference between Gaussian, Multinomial, and Bernoulli Naive Bayes\"\n",
        "- \"How does Laplace smoothing work in Naive Bayes?\"\n",
        "- \"When would you use Naive Bayes over other algorithms?\"\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've mastered k-Nearest Neighbors and Naive Bayes - two fundamental algorithms that approach machine learning from completely different perspectives. You now understand both similarity-based learning and probabilistic classification.\n",
        "\n",
        "**Remember:** The key to mastering these algorithms is understanding their strengths and weaknesses. kNN is powerful but computationally expensive, while Naive Bayes is fast and often surprisingly effective despite its \"naive\" assumptions. Tomorrow we'll learn how to combine multiple algorithms to create even more powerful ensemble methods.\n",
        "\n",
        "**Keep practicing:** Try implementing these algorithms on your own datasets and always ask yourself: \"Why does this algorithm work well (or poorly) on this specific problem?\" This analytical thinking will make you a better data scientist.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
