{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 16: Building Neural Networks with PyTorch - From Theory to Practice\n",
        "\n",
        "**Welcome to Day 16 of your ML journey!** Today we transition from understanding neural network theory to building them in practice using **PyTorch**, one of the most popular and powerful deep learning frameworks. You'll learn to create, train, and evaluate neural networks for real-world problems.\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Master PyTorch fundamentals and build your first neural network from scratch, understanding the complete workflow from data preparation to model deployment.\n",
        "\n",
        "**Topics Covered:**\n",
        "- PyTorch fundamentals: tensors, autograd, and neural network modules\n",
        "- Building neural networks with nn.Module and nn.Sequential\n",
        "- Training loops: forward pass, loss calculation, backpropagation\n",
        "- Data handling with DataLoader and Dataset classes\n",
        "- Model evaluation and visualization techniques\n",
        "- Best practices for neural network development\n",
        "- Real-world classification and regression examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Concept Overview\n",
        "\n",
        "### What is PyTorch?\n",
        "\n",
        "**PyTorch** is an open-source machine learning library developed by Facebook's AI Research lab. It's designed to be intuitive, flexible, and efficient for both research and production use. PyTorch has become the framework of choice for many researchers and practitioners due to its dynamic computation graph and Pythonic interface.\n",
        "\n",
        "**Key Advantages of PyTorch:**\n",
        "1. **Dynamic Computation Graphs**: Build and modify networks on-the-fly\n",
        "2. **Pythonic Design**: Feels natural to Python developers\n",
        "3. **Strong Community**: Extensive ecosystem and pre-trained models\n",
        "4. **Production Ready**: TorchScript for deployment optimization\n",
        "5. **Research Friendly**: Easy experimentation and rapid prototyping\n",
        "\n",
        "### Core PyTorch Components\n",
        "\n",
        "| Component | Purpose | Key Features |\n",
        "|-----------|---------|--------------|\n",
        "| **Tensors** | Multi-dimensional arrays | GPU acceleration, automatic differentiation |\n",
        "| **Autograd** | Automatic differentiation | Gradient computation, backpropagation |\n",
        "| **nn.Module** | Neural network building blocks | Reusable layers, parameter management |\n",
        "| **Optimizers** | Training algorithms | SGD, Adam, RMSprop, and more |\n",
        "| **Loss Functions** | Training objectives | Cross-entropy, MSE, custom losses |\n",
        "\n",
        "### Neural Network Architecture in PyTorch\n",
        "\n",
        "**Building Blocks:**\n",
        "- **Input Layer**: Receives your data\n",
        "- **Hidden Layers**: Learn complex patterns (fully connected, convolutional, etc.)\n",
        "- **Activation Functions**: Introduce non-linearity (ReLU, sigmoid, tanh)\n",
        "- **Output Layer**: Produces predictions\n",
        "- **Loss Function**: Measures prediction quality\n",
        "- **Optimizer**: Updates model parameters\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **Image Classification**: Recognizing objects in photos\n",
        "- **Natural Language Processing**: Sentiment analysis, text generation\n",
        "- **Time Series Forecasting**: Stock prices, weather prediction\n",
        "- **Recommendation Systems**: Personalized content suggestions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Code Demo: PyTorch Fundamentals\n",
        "\n",
        "Let's start by exploring PyTorch's core components and building our first neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "Using CPU\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check PyTorch version and device availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Understanding PyTorch Tensors\n",
        "\n",
        "Tensors are the fundamental data structure in PyTorch - think of them as multi-dimensional arrays with GPU acceleration and automatic differentiation capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating Tensors ===\n",
            "From list: tensor([1, 2, 3, 4, 5])\n",
            "From NumPy:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]], dtype=torch.int32)\n",
            "Random normal (3x4):\n",
            "tensor([[-0.3267, -0.2788, -0.4220, -1.3323],\n",
            "        [-0.3639,  0.1513, -0.3514, -0.7906],\n",
            "        [-0.0915,  0.2352,  2.2440,  0.5817]])\n",
            "Random uniform (2x3):\n",
            "tensor([[0.6440, 0.7071, 0.6581],\n",
            "        [0.4913, 0.8913, 0.1447]])\n",
            "\n",
            "=== Tensor Operations ===\n",
            "Matrix A:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "Matrix B:\n",
            "tensor([[5., 6.],\n",
            "        [7., 8.]])\n",
            "A + B:\n",
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]])\n",
            "A * B (element-wise):\n",
            "tensor([[ 5., 12.],\n",
            "        [21., 32.]])\n",
            "A @ B (matrix multiplication):\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "A.shape: torch.Size([2, 2])\n",
            "A.dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Create tensors from different sources\n",
        "print(\"=== Creating Tensors ===\")\n",
        "\n",
        "# From Python list\n",
        "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
        "print(f\"From list: {tensor_from_list}\")\n",
        "\n",
        "# From NumPy array\n",
        "numpy_array = np.array([[1, 2], [3, 4]])\n",
        "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
        "print(f\"From NumPy:\\n{tensor_from_numpy}\")\n",
        "\n",
        "# Random tensors\n",
        "random_tensor = torch.randn(3, 4)  # Normal distribution\n",
        "uniform_tensor = torch.rand(2, 3)  # Uniform distribution [0, 1]\n",
        "print(f\"Random normal (3x4):\\n{random_tensor}\")\n",
        "print(f\"Random uniform (2x3):\\n{uniform_tensor}\")\n",
        "\n",
        "# Tensor operations\n",
        "print(\"\\n=== Tensor Operations ===\")\n",
        "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "\n",
        "print(f\"Matrix A:\\n{a}\")\n",
        "print(f\"Matrix B:\\n{b}\")\n",
        "print(f\"A + B:\\n{a + b}\")\n",
        "print(f\"A * B (element-wise):\\n{a * b}\")\n",
        "print(f\"A @ B (matrix multiplication):\\n{a @ b}\")\n",
        "print(f\"A.shape: {a.shape}\")\n",
        "print(f\"A.dtype: {a.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Automatic Differentiation with Autograd\n",
        "\n",
        "PyTorch's autograd system automatically computes gradients, which is essential for training neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Automatic Differentiation ===\n",
            "x = 2.0, w = 3.0, b = 1.0\n",
            "y = w*x + b = 7.0\n",
            "\n",
            "Gradients:\n",
            "dy/dx = 3.0\n",
            "dy/dw = 2.0\n",
            "dy/db = 1.0\n",
            "\n",
            "=== Complex Function Example ===\n",
            "y = x² + 2x + 1 = 16.0\n",
            "dy/dx = 8.0\n"
          ]
        }
      ],
      "source": [
        "# Understanding autograd\n",
        "print(\"=== Automatic Differentiation ===\")\n",
        "\n",
        "# Create tensors with requires_grad=True for gradient computation\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "print(f\"x = {x}, w = {w}, b = {b}\")\n",
        "\n",
        "# Define a simple function: y = w*x + b\n",
        "y = w * x + b\n",
        "print(f\"y = w*x + b = {y}\")\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "print(f\"\\nGradients:\")\n",
        "print(f\"dy/dx = {x.grad}\")  # Should be 3.0 (w)\n",
        "print(f\"dy/dw = {w.grad}\")  # Should be 2.0 (x)\n",
        "print(f\"dy/db = {b.grad}\")  # Should be 1.0\n",
        "\n",
        "# More complex example: y = x^2 + 2x + 1\n",
        "print(\"\\n=== Complex Function Example ===\")\n",
        "x2 = torch.tensor(3.0, requires_grad=True)\n",
        "y2 = x2**2 + 2*x2 + 1\n",
        "print(f\"y = x² + 2x + 1 = {y2}\")\n",
        "\n",
        "y2.backward()\n",
        "print(f\"dy/dx = {x2.grad}\")  # Should be 2*3 + 2 = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Building Your First Neural Network\n",
        "\n",
        "Now let's build a simple neural network for binary classification using the classic approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Dataset Preparation ===\n",
            "Dataset shape: torch.Size([1000, 20])\n",
            "Target shape: torch.Size([1000, 1])\n",
            "Class distribution: [497 503]\n",
            "\n",
            "Training set: 800 samples\n",
            "Test set: 200 samples\n"
          ]
        }
      ],
      "source": [
        "# Generate synthetic dataset for binary classification\n",
        "print(\"=== Dataset Preparation ===\")\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.FloatTensor(y).unsqueeze(1)  # Add dimension for batch processing\n",
        "\n",
        "print(f\"Dataset shape: {X_tensor.shape}\")\n",
        "print(f\"Target shape: {y_tensor.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Neural Network Architecture ===\n",
            "SimpleNeuralNetwork(\n",
            "  (fc1): Linear(in_features=20, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "\n",
            "Total parameters: 5,569\n",
            "Trainable parameters: 5,569\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Building Neural Network using nn.Module (Recommended)\n",
        "class SimpleNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNeuralNetwork, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Hidden to hidden\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)  # Hidden to output\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = F.relu(self.fc1(x))  # First hidden layer with ReLU\n",
        "        x = self.dropout(x)      # Apply dropout\n",
        "        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU\n",
        "        x = self.dropout(x)      # Apply dropout\n",
        "        x = torch.sigmoid(self.fc3(x))  # Output layer with sigmoid\n",
        "        return x\n",
        "\n",
        "# Create model instance\n",
        "model = SimpleNeuralNetwork(\n",
        "    input_size=20,    # Number of features\n",
        "    hidden_size=64,   # Hidden layer size\n",
        "    output_size=1     # Binary classification\n",
        ")\n",
        "\n",
        "print(\"=== Neural Network Architecture ===\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Sequential Model ===\n",
            "Sequential(\n",
            "  (0): Linear(in_features=20, out_features=64, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.2, inplace=False)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (7): Sigmoid()\n",
            ")\n",
            "\n",
            "Sequential model parameters: 5,569\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Building Neural Network using nn.Sequential (Alternative)\n",
        "sequential_model = nn.Sequential(\n",
        "    nn.Linear(20, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(64, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "print(\"=== Sequential Model ===\")\n",
        "print(sequential_model)\n",
        "\n",
        "# Both models have the same architecture\n",
        "print(f\"\\nSequential model parameters: {sum(p.numel() for p in sequential_model.parameters()):,}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
