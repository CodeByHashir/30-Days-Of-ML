{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 17: Convolutional Neural Networks (CNNs) - Image Classification Mastery\n",
        "\n",
        "**Welcome to Day 17 of your ML journey!** Today we dive into one of the most revolutionary architectures in deep learning: **Convolutional Neural Networks (CNNs)**. Building on your solid PyTorch foundation from Day 16, you'll now learn to build models that can \"see\" and understand images with superhuman accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Master CNN architecture and build production-ready image classification systems using PyTorch.\n",
        "\n",
        "**Topics Covered:**\n",
        "- CNN architecture: convolution, pooling, and feature learning\n",
        "- Building CNNs from scratch with PyTorch\n",
        "- Image preprocessing and data augmentation\n",
        "- Training CNNs on MNIST and CIFAR-10 datasets\n",
        "- Feature visualization and model interpretation\n",
        "- Advanced techniques: batch normalization, dropout, residual connections\n",
        "- Transfer learning fundamentals\n",
        "- Real-world applications and industry best practices\n",
        "\n",
        "**Real-World Impact:** CNNs power everything from medical diagnosis to autonomous vehicles, social media filters to security systems. By the end of today, you'll understand the technology behind these applications and be able to build your own image recognition systems.\n",
        "\n",
        "**Prerequisites:** Solid understanding of PyTorch fundamentals (Day 16), neural network basics (Day 15), and Python programming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Concept Overview: Understanding CNNs\n",
        "\n",
        "### What are Convolutional Neural Networks?\n",
        "\n",
        "**Convolutional Neural Networks (CNNs)** are specialized neural networks designed to process data with a grid-like topology, such as images. They're inspired by the visual cortex of animals and are exceptionally effective at recognizing patterns in visual data.\n",
        "\n",
        "**The Core Intuition:**\n",
        "Think of CNNs like a team of specialized detectives examining a crime scene photo. Each detective (filter) looks for specific clues (features) - one might focus on edges, another on textures, another on shapes. They work together to piece together the complete picture.\n",
        "\n",
        "**Why CNNs Excel at Images:**\n",
        "1. **Spatial Relationships**: Preserves the 2D structure of images\n",
        "2. **Parameter Sharing**: Same filters applied across the entire image\n",
        "3. **Translation Invariance**: Recognizes objects regardless of position\n",
        "4. **Hierarchical Learning**: Low-level features → High-level concepts\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **Medical Imaging**: Detecting tumors, analyzing X-rays, diagnosing diseases\n",
        "- **Autonomous Vehicles**: Recognizing traffic signs, pedestrians, other vehicles\n",
        "- **Social Media**: Face recognition, content moderation, photo enhancement\n",
        "- **Security**: Surveillance systems, biometric authentication\n",
        "- **E-commerce**: Product recognition, visual search, quality control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN Building Blocks Explained\n",
        "\n",
        "CNNs consist of several key components that work together to process images:\n",
        "\n",
        "#### 1. **Convolutional Layers**\n",
        "The heart of CNNs. These layers apply filters (kernels) to detect features:\n",
        "\n",
        "**How Convolution Works:**\n",
        "- A small filter (e.g., 3×3) slides across the image\n",
        "- At each position, it performs element-wise multiplication\n",
        "- Results are summed to produce a single value in the output feature map\n",
        "- Different filters detect different features (edges, textures, patterns)\n",
        "\n",
        "**Key Parameters:**\n",
        "- **Filter Size**: Typically 3×3 or 5×5 (larger = more context)\n",
        "- **Stride**: How many pixels the filter moves (1 = every pixel, 2 = every other pixel)\n",
        "- **Padding**: Adding zeros around the image to preserve size\n",
        "- **Number of Filters**: More filters = more feature types detected\n",
        "\n",
        "#### 2. **Activation Functions**\n",
        "Introduce non-linearity to enable complex pattern learning:\n",
        "- **ReLU (Rectified Linear Unit)**: Most common, f(x) = max(0, x)\n",
        "- **Leaky ReLU**: Fixes \"dying ReLU\" problem\n",
        "- **ELU**: Smooth alternative with better gradient flow\n",
        "\n",
        "#### 3. **Pooling Layers**\n",
        "Reduce spatial dimensions while preserving important information:\n",
        "- **Max Pooling**: Takes maximum value in each region (most common)\n",
        "- **Average Pooling**: Takes average value in each region\n",
        "- **Benefits**: Reduces overfitting, computational cost, and parameters\n",
        "\n",
        "#### 4. **Fully Connected Layers**\n",
        "Traditional neural network layers at the end:\n",
        "- Flatten feature maps into vectors\n",
        "- Perform final classification or regression\n",
        "- Can include dropout for regularization\n",
        "\n",
        "**Visual Suggestion**: CNN Architecture Diagram showing the flow from input image through conv layers, pooling, and fully connected layers to final prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Convolution Works: A Deep Dive\n",
        "\n",
        "**The Convolution Operation:**\n",
        "Convolution is a mathematical operation that combines two functions to produce a third function. In CNNs, we use discrete convolution:\n",
        "\n",
        "```\n",
        "Output[i,j] = Σ Σ Input[i+m, j+n] × Filter[m, n]\n",
        "              m n\n",
        "```\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "1. **Place Filter**: Position the filter over a region of the input\n",
        "2. **Element-wise Multiply**: Multiply corresponding elements\n",
        "3. **Sum Results**: Add all products together\n",
        "4. **Store Output**: Place result in corresponding position of output\n",
        "5. **Slide Filter**: Move filter to next position and repeat\n",
        "\n",
        "**Feature Detection Examples:**\n",
        "- **Edge Detection**: Filters that detect horizontal, vertical, diagonal edges\n",
        "- **Texture Detection**: Filters that identify patterns like wood grain, fabric\n",
        "- **Shape Detection**: Filters that recognize circles, squares, triangles\n",
        "- **Color Patterns**: Filters that detect specific color combinations\n",
        "\n",
        "**Hierarchical Learning:**\n",
        "- **Layer 1**: Detects edges, corners, basic shapes\n",
        "- **Layer 2**: Combines edges into textures, simple shapes\n",
        "- **Layer 3**: Recognizes object parts (eyes, wheels, doors)\n",
        "- **Layer 4+**: Identifies complete objects (faces, cars, buildings)\n",
        "\n",
        "**Visual Suggestion**: Animation showing a 3×3 filter sliding across a 5×5 image, with element-wise multiplication and summation at each step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pooling and Dimensionality Reduction\n",
        "\n",
        "**Why Pooling is Essential:**\n",
        "Pooling layers serve multiple critical purposes in CNNs:\n",
        "\n",
        "1. **Dimensionality Reduction**: Reduces spatial size of feature maps\n",
        "2. **Translation Invariance**: Makes the network robust to small shifts\n",
        "3. **Computational Efficiency**: Reduces parameters and computation\n",
        "4. **Overfitting Prevention**: Acts as a form of regularization\n",
        "\n",
        "**Max Pooling (Most Common):**\n",
        "- Takes the maximum value in each pooling region\n",
        "- Preserves the strongest activation (most important feature)\n",
        "- Commonly uses 2×2 pooling with stride 2\n",
        "- Reduces spatial dimensions by half\n",
        "\n",
        "**Average Pooling:**\n",
        "- Takes the average value in each pooling region\n",
        "- Smoother output, less sensitive to outliers\n",
        "- Sometimes used in final layers for global pooling\n",
        "\n",
        "**Global Pooling:**\n",
        "- Reduces entire feature map to single value\n",
        "- Global Average Pooling (GAP) popular in modern architectures\n",
        "- Eliminates need for fully connected layers\n",
        "\n",
        "**Spatial Invariance Benefits:**\n",
        "- Object recognition regardless of exact position\n",
        "- Robustness to small translations and rotations\n",
        "- Better generalization to new data\n",
        "\n",
        "**Visual Suggestion**: Side-by-side comparison showing how max pooling and average pooling work on the same feature map, with before/after dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Code Demo: Building CNNs with PyTorch\n",
        "\n",
        "Let's dive into practical implementation! We'll start with a simple CNN and progressively build more sophisticated architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check PyTorch version and device availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Dataset Loading and Exploration\n",
        "\n",
        "Let's start with the MNIST dataset - a classic benchmark for image classification. MNIST contains 70,000 grayscale images of handwritten digits (0-9), each 28×28 pixels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data transformations\n",
        "# Convert PIL images to tensors and normalize pixel values to [0,1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL Image to tensor and scale to [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1,1] range\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "print(\"Loading MNIST dataset...\")\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images from the dataset\n",
        "def visualize_samples(dataset, num_samples=16):\n",
        "    \"\"\"Visualize sample images from the dataset\"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        image, label = dataset[i]\n",
        "        # Denormalize image for visualization\n",
        "        image = image * 0.5 + 0.5  # Convert from [-1,1] back to [0,1]\n",
        "        \n",
        "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f'Label: {label}', fontsize=12)\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize training samples\n",
        "print(\"Sample images from MNIST training set:\")\n",
        "visualize_samples(train_dataset)\n",
        "\n",
        "# Analyze class distribution\n",
        "def analyze_class_distribution(dataset, title):\n",
        "    \"\"\"Analyze and visualize class distribution\"\"\"\n",
        "    labels = [dataset[i][1] for i in range(len(dataset))]\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    # Bar plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    bars = plt.bar(unique, counts, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'{title} - Class Distribution')\n",
        "    plt.xlabel('Digit Class')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.xticks(unique)\n",
        "    \n",
        "    # Add count labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "                str(count), ha='center', va='bottom')\n",
        "    \n",
        "    # Pie chart\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.pie(counts, labels=unique, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title(f'{title} - Class Proportions')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n{title} Statistics:\")\n",
        "    for digit, count in zip(unique, counts):\n",
        "        print(f\"Digit {digit}: {count:,} samples ({count/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "# Analyze both training and test sets\n",
        "analyze_class_distribution(train_dataset, \"Training Set\")\n",
        "analyze_class_distribution(test_dataset, \"Test Set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Building a Simple CNN from Scratch\n",
        "\n",
        "Let's create our first CNN! We'll build a simple but effective architecture with two convolutional layers, followed by pooling and fully connected layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple CNN for MNIST digit classification.\n",
        "    Architecture: Conv2d → ReLU → MaxPool → Conv2d → ReLU → MaxPool → Flatten → FC → ReLU → Dropout → FC\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # First convolutional block\n",
        "        # Input: 1 channel (grayscale), Output: 32 feature maps\n",
        "        # Kernel size 3x3, padding 1 to preserve spatial dimensions\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces 28x28 to 14x14\n",
        "        \n",
        "        # Second convolutional block\n",
        "        # Input: 32 channels, Output: 64 feature maps\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces 14x14 to 7x7\n",
        "        \n",
        "        # Calculate the size after conv and pooling layers\n",
        "        # 28x28 → 14x14 → 7x7, so 7*7*64 = 3136 features\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 128)  # First fully connected layer\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
        "        self.fc2 = nn.Linear(128, num_classes)  # Output layer (10 classes for MNIST)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # First conv block: 1x28x28 → 32x28x28 → 32x14x14\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        \n",
        "        # Second conv block: 32x14x14 → 64x14x14 → 64x7x7\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        \n",
        "        # Flatten for fully connected layers: 64x7x7 → 3136\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected layers: 3136 → 128 → 10\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create model instance and move to device\n",
        "model = SimpleCNN(num_classes=10).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"Simple CNN Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Print layer-wise parameter count\n",
        "print(\"\\nLayer-wise Parameter Count:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.numel():,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Training Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adam optimizer with L2 regularization\n",
        "\n",
        "# Learning rate scheduler - reduces learning rate when validation loss plateaus\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Training configuration\n",
        "num_epochs = 10\n",
        "print_every = 100  # Print progress every 100 batches\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer}\")\n",
        "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "# Initialize lists to store training history\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Complete Training Loop with Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Progress bar for training\n",
        "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # Move data to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, test_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "        pbar = tqdm(test_loader, desc=\"Validation\", leave=False)\n",
        "        \n",
        "        for data, target in pbar:\n",
        "            # Move data to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(test_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, test_loader, criterion, device)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Visualizing Training Progress and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive training visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Training and Validation Loss\n",
        "axes[0, 0].plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
        "axes[0, 0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training and Validation Accuracy\n",
        "axes[0, 1].plot(train_accuracies, label='Training Accuracy', color='blue', linewidth=2)\n",
        "axes[0, 1].plot(val_accuracies, label='Validation Accuracy', color='red', linewidth=2)\n",
        "axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Learning Rate Schedule\n",
        "axes[1, 0].plot(learning_rates, color='green', linewidth=2, marker='o')\n",
        "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Learning Rate')\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Final Performance Summary\n",
        "epochs = range(1, len(train_accuracies) + 1)\n",
        "final_train_acc = train_accuracies[-1]\n",
        "final_val_acc = val_accuracies[-1]\n",
        "final_train_loss = train_losses[-1]\n",
        "final_val_loss = val_losses[-1]\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "Final Results:\n",
        "Training Accuracy: {final_train_acc:.2f}%\n",
        "Validation Accuracy: {final_val_acc:.2f}%\n",
        "Training Loss: {final_train_loss:.4f}\n",
        "Validation Loss: {final_val_loss:.4f}\n",
        "Training Time: {training_time:.2f}s\n",
        "Total Parameters: {total_params:,}\n",
        "\"\"\"\n",
        "\n",
        "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
        "axes[1, 1].set_title('Training Summary', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed results\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
        "print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Parameters per Image: {total_params/len(train_dataset):.1f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7 Model Evaluation and Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set and create confusion matrix\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model and return predictions and true labels\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    \n",
        "    return np.array(all_predictions), np.array(all_targets)\n",
        "\n",
        "# Get predictions\n",
        "predictions, true_labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Overall Test Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Confusion Matrix Heatmap\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Plot 2: Per-class Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "bars = plt.bar(range(10), per_class_accuracy, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Digit Class')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(range(10))\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for i, (bar, acc) in enumerate(zip(bars, per_class_accuracy)):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(true_labels, predictions, target_names=[str(i) for i in range(10)]))\n",
        "\n",
        "# Find misclassified examples\n",
        "misclassified_indices = np.where(predictions != true_labels)[0]\n",
        "print(f\"\\nMisclassified samples: {len(misclassified_indices)} out of {len(true_labels)} ({len(misclassified_indices)/len(true_labels)*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.8 Sample Predictions and Misclassification Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample predictions\n",
        "def visualize_predictions(model, test_dataset, predictions, true_labels, num_samples=16):\n",
        "    \"\"\"Visualize sample predictions with confidence scores\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get random sample of test data\n",
        "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, true_label = test_dataset[idx]\n",
        "            image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
        "            \n",
        "            # Get prediction and confidence\n",
        "            output = model(image)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            predicted_label = output.argmax().item()\n",
        "            confidence = probabilities[0][predicted_label].item()\n",
        "            \n",
        "            # Denormalize image for visualization\n",
        "            image_display = image.squeeze().cpu() * 0.5 + 0.5\n",
        "            \n",
        "            # Plot image\n",
        "            axes[i].imshow(image_display.squeeze(), cmap='gray')\n",
        "            \n",
        "            # Color code: green for correct, red for incorrect\n",
        "            color = 'green' if predicted_label == true_label else 'red'\n",
        "            axes[i].set_title(f'True: {true_label}, Pred: {predicted_label}\\nConf: {confidence:.3f}', \n",
        "                            color=color, fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize random sample predictions\n",
        "print(\"Sample Predictions (Green=Correct, Red=Incorrect):\")\n",
        "visualize_predictions(model, test_dataset, predictions, true_labels)\n",
        "\n",
        "# Analyze misclassified examples\n",
        "if len(misclassified_indices) > 0:\n",
        "    print(f\"\\nAnalyzing {min(16, len(misclassified_indices))} misclassified examples:\")\n",
        "    \n",
        "    # Get sample of misclassified examples\n",
        "    sample_misclassified = np.random.choice(misclassified_indices, \n",
        "                                           min(16, len(misclassified_indices)), \n",
        "                                           replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, idx in enumerate(sample_misclassified):\n",
        "        image, true_label = test_dataset[idx]\n",
        "        predicted_label = predictions[idx]\n",
        "        \n",
        "        # Denormalize image\n",
        "        image_display = image * 0.5 + 0.5\n",
        "        \n",
        "        axes[i].imshow(image_display.squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f'True: {true_label}, Pred: {predicted_label}', \n",
        "                         color='red', fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(sample_misclassified), 16):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze common misclassification patterns\n",
        "    print(\"\\nCommon Misclassification Patterns:\")\n",
        "    misclass_pairs = []\n",
        "    for idx in misclassified_indices:\n",
        "        true_label = true_labels[idx]\n",
        "        pred_label = predictions[idx]\n",
        "        misclass_pairs.append((true_label, pred_label))\n",
        "    \n",
        "    from collections import Counter\n",
        "    common_mistakes = Counter(misclass_pairs).most_common(5)\n",
        "    \n",
        "    for (true_label, pred_label), count in common_mistakes:\n",
        "        print(f\"True {true_label} → Predicted {pred_label}: {count} times\")\n",
        "else:\n",
        "    print(\"No misclassified examples found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Advanced CNN Architecture for CIFAR-10\n",
        "\n",
        "Now let's tackle a more challenging dataset: CIFAR-10. This dataset contains 60,000 color images (32×32 pixels) of 10 different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 CIFAR-10 Dataset Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data augmentation transforms for CIFAR-10\n",
        "# Data augmentation helps prevent overfitting and improves generalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(10),  # Random rotation up to 10 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random color adjustments\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels\n",
        "])\n",
        "\n",
        "# No augmentation for test set\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "cifar_train_dataset = datasets.CIFAR10(\n",
        "    root='./data', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "cifar_test_dataset = datasets.CIFAR10(\n",
        "    root='./data', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "cifar_batch_size = 128  # Larger batch size for CIFAR-10\n",
        "cifar_train_loader = DataLoader(cifar_train_dataset, batch_size=cifar_batch_size, shuffle=True)\n",
        "cifar_test_loader = DataLoader(cifar_test_dataset, batch_size=cifar_batch_size, shuffle=False)\n",
        "\n",
        "print(f\"CIFAR-10 Training samples: {len(cifar_train_dataset)}\")\n",
        "print(f\"CIFAR-10 Test samples: {len(cifar_test_dataset)}\")\n",
        "print(f\"Image shape: {cifar_train_dataset[0][0].shape}\")\n",
        "print(f\"Number of classes: {len(cifar_train_dataset.classes)}\")\n",
        "print(f\"Classes: {cifar_train_dataset.classes}\")\n",
        "\n",
        "# Visualize CIFAR-10 samples\n",
        "def visualize_cifar_samples(dataset, num_samples=16):\n",
        "    \"\"\"Visualize sample images from CIFAR-10 dataset\"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        image, label = dataset[i]\n",
        "        # Denormalize image for visualization\n",
        "        image = image * 0.5 + 0.5  # Convert from [-1,1] back to [0,1]\n",
        "        \n",
        "        # Convert tensor to numpy and transpose for matplotlib\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "        \n",
        "        axes[i].imshow(image_np)\n",
        "        axes[i].set_title(f'{dataset.classes[label]}', fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nSample images from CIFAR-10 training set:\")\n",
        "visualize_cifar_samples(cifar_train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Advanced CNN Architecture with Modern Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Advanced CNN architecture for CIFAR-10 classification.\n",
        "    Features: Batch Normalization, Dropout, Residual Connections, Modern Architecture\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        \n",
        "        # First convolutional block with batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # 3 channels for RGB\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 32x32 → 16x16\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        \n",
        "        # Second convolutional block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 16x16 → 8x8\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "        \n",
        "        # Third convolutional block\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # 8x8 → 4x4\n",
        "        self.dropout3 = nn.Dropout2d(0.25)\n",
        "        \n",
        "        # Global Average Pooling instead of fully connected layers\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # 4x4 → 1x1\n",
        "        \n",
        "        # Final classification layers\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the advanced CNN\"\"\"\n",
        "        # First block: 3x32x32 → 64x16x16\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        # Second block: 64x16x16 → 128x8x8\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        # Third block: 128x8x8 → 256x4x4\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        # Global average pooling: 256x4x4 → 256x1x1\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten: 256x1x1 → 256\n",
        "        \n",
        "        # Final classification: 256 → 512 → 10\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create advanced model\n",
        "advanced_model = AdvancedCNN(num_classes=10).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"Advanced CNN Architecture:\")\n",
        "print(advanced_model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in advanced_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in advanced_model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Compare with simple CNN\n",
        "simple_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Simple CNN parameters: {simple_params:,}\")\n",
        "print(f\"Parameter increase: {total_params/simple_params:.1f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Training the Advanced CNN on CIFAR-10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training for advanced CNN\n",
        "cifar_criterion = nn.CrossEntropyLoss()\n",
        "cifar_optimizer = optim.Adam(advanced_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "cifar_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    cifar_optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# Training configuration\n",
        "cifar_epochs = 20  # More epochs for CIFAR-10\n",
        "\n",
        "# Initialize training history\n",
        "cifar_train_losses = []\n",
        "cifar_train_accuracies = []\n",
        "cifar_val_losses = []\n",
        "cifar_val_accuracies = []\n",
        "cifar_learning_rates = []\n",
        "\n",
        "print(\"Starting CIFAR-10 training...\")\n",
        "print(f\"Model: Advanced CNN with {total_params:,} parameters\")\n",
        "print(f\"Epochs: {cifar_epochs}\")\n",
        "print(f\"Batch size: {cifar_batch_size}\")\n",
        "print(f\"Learning rate: {cifar_optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(cifar_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{cifar_epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(advanced_model, cifar_train_loader, cifar_criterion, cifar_optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(advanced_model, cifar_test_loader, cifar_criterion, device)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    cifar_scheduler.step(val_loss)\n",
        "    \n",
        "    # Store metrics\n",
        "    cifar_train_losses.append(train_loss)\n",
        "    cifar_train_accuracies.append(train_acc)\n",
        "    cifar_val_losses.append(val_loss)\n",
        "    cifar_val_accuracies.append(val_acc)\n",
        "    cifar_learning_rates.append(cifar_optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {cifar_optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "cifar_training_time = time.time() - start_time\n",
        "print(f\"\\nCIFAR-10 training completed in {cifar_training_time:.2f} seconds\")\n",
        "print(f\"Final validation accuracy: {cifar_val_accuracies[-1]:.2f}%\")\n",
        "print(f\"Best validation accuracy: {max(cifar_val_accuracies):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Hands-on Exercises: CNN Mastery\n",
        "\n",
        "Now it's your turn to experiment and deepen your understanding! These exercises will help you master CNN concepts through practical implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Architecture Experimentation\n",
        "\n",
        "**Task**: Modify the CNN architecture and observe the impact on performance.\n",
        "\n",
        "**Progressive Hints:**\n",
        "- **Level 1**: Change the number of filters in conv layers (try 16, 32, 64, 128)\n",
        "- **Level 2**: Add more convolutional layers (3rd, 4th conv block)\n",
        "- **Level 3**: Experiment with different kernel sizes (3x3 vs 5x5)\n",
        "- **Level 4**: Try different pooling strategies (max vs average pooling)\n",
        "\n",
        "**Advanced Requirements:**\n",
        "- Compare training time vs accuracy trade-offs\n",
        "- Analyze parameter count vs performance relationship\n",
        "- Visualize the impact on feature maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercise 1 - Architecture Experimentation\n",
        "print(\"=== Exercise 1: Architecture Experimentation ===\")\n",
        "\n",
        "# TODO: Level 1 - Create different filter configurations\n",
        "def create_cnn_variant(filters_config, name):\n",
        "    \"\"\"\n",
        "    Create CNN with different filter configurations\n",
        "    filters_config: list of filter counts for each conv layer\n",
        "    \"\"\"\n",
        "    class CustomCNN(nn.Module):\n",
        "        def __init__(self, num_classes=10):\n",
        "            super(CustomCNN, self).__init__()\n",
        "            \n",
        "            # TODO: Implement dynamic architecture based on filters_config\n",
        "            # Use filters_config to determine number of filters in each layer\n",
        "            # Example: filters_config = [32, 64] for 2 conv layers\n",
        "            \n",
        "            pass\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # TODO: Implement forward pass\n",
        "            pass\n",
        "    \n",
        "    return CustomCNN\n",
        "\n",
        "# TODO: Level 2 - Test different architectures\n",
        "architectures = {\n",
        "    'Small': [16, 32],      # Fewer parameters\n",
        "    'Medium': [32, 64],     # Original\n",
        "    'Large': [64, 128],     # More parameters\n",
        "    'Extra_Large': [128, 256]  # Many parameters\n",
        "}\n",
        "\n",
        "# TODO: Level 3 - Train and compare architectures\n",
        "results = {}\n",
        "\n",
        "for name, filters in architectures.items():\n",
        "    print(f\"\\nTesting {name} architecture with filters: {filters}\")\n",
        "    \n",
        "    # TODO: Create model\n",
        "    # model = create_cnn_variant(filters, name)().to(device)\n",
        "    \n",
        "    # TODO: Train model (use fewer epochs for quick testing)\n",
        "    # train_loss, train_acc = train_model(model, epochs=5)\n",
        "    \n",
        "    # TODO: Store results\n",
        "    # results[name] = {'filters': filters, 'accuracy': train_acc, 'params': count_params(model)}\n",
        "    \n",
        "    print(f\"TODO: Implement {name} architecture testing\")\n",
        "\n",
        "# TODO: Level 4 - Visualize results\n",
        "# Create comparison plots showing:\n",
        "# - Accuracy vs Parameter count\n",
        "# - Training time vs Accuracy\n",
        "# - Architecture complexity vs Performance\n",
        "\n",
        "print(\"\\nExercise 1: Architecture experimentation framework ready!\")\n",
        "print(\"Implement the TODO sections to complete the exercise.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Data Augmentation Impact Analysis\n",
        "\n",
        "**Task**: Implement different data augmentation strategies and measure their impact on model performance.\n",
        "\n",
        "**Progressive Hints:**\n",
        "- **Level 1**: Compare no augmentation vs basic augmentation (horizontal flip)\n",
        "- **Level 2**: Add rotation and color jittering\n",
        "- **Level 3**: Implement advanced augmentations (cutout, mixup)\n",
        "- **Level 4**: Analyze overfitting reduction and generalization improvement\n",
        "\n",
        "**Advanced Requirements:**\n",
        "- Measure training vs validation accuracy gap\n",
        "- Visualize augmented samples\n",
        "- Compare training curves with/without augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercise 2 - Data Augmentation Impact Analysis\n",
        "print(\"=== Exercise 2: Data Augmentation Impact Analysis ===\")\n",
        "\n",
        "# TODO: Level 1 - Define different augmentation strategies\n",
        "augmentation_strategies = {\n",
        "    'No_Augmentation': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]),\n",
        "    \n",
        "    'Basic_Augmentation': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]),\n",
        "    \n",
        "    'Moderate_Augmentation': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]),\n",
        "    \n",
        "    'Heavy_Augmentation': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "}\n",
        "\n",
        "# TODO: Level 2 - Visualize augmented samples\n",
        "def visualize_augmentations(dataset, transform, num_samples=8):\n",
        "    \"\"\"Visualize how different augmentations affect the same image\"\"\"\n",
        "    # TODO: Implement augmentation visualization\n",
        "    # Show original image and several augmented versions\n",
        "    pass\n",
        "\n",
        "# TODO: Level 3 - Train models with different augmentations\n",
        "augmentation_results = {}\n",
        "\n",
        "for strategy_name, transform in augmentation_strategies.items():\n",
        "    print(f\"\\nTesting {strategy_name}...\")\n",
        "    \n",
        "    # TODO: Create datasets with different augmentations\n",
        "    # train_dataset_aug = datasets.CIFAR10(..., transform=transform)\n",
        "    # train_loader_aug = DataLoader(train_dataset_aug, ...)\n",
        "    \n",
        "    # TODO: Train model with this augmentation strategy\n",
        "    # model_aug = SimpleCNN().to(device)\n",
        "    # train_loss, train_acc, val_loss, val_acc = train_model(model_aug, train_loader_aug, epochs=10)\n",
        "    \n",
        "    # TODO: Store results\n",
        "    # augmentation_results[strategy_name] = {\n",
        "    #     'train_acc': train_acc,\n",
        "    #     'val_acc': val_acc,\n",
        "    #     'overfitting_gap': train_acc - val_acc\n",
        "    # }\n",
        "    \n",
        "    print(f\"TODO: Implement {strategy_name} testing\")\n",
        "\n",
        "# TODO: Level 4 - Analyze and visualize results\n",
        "# Create plots showing:\n",
        "# - Training accuracy vs Validation accuracy for each strategy\n",
        "# - Overfitting gap comparison\n",
        "# - Sample augmented images\n",
        "\n",
        "print(\"\\nExercise 2: Data augmentation analysis framework ready!\")\n",
        "print(\"Implement the TODO sections to complete the exercise.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Custom Dataset Challenge\n",
        "\n",
        "**Task**: Apply your CNN knowledge to a new dataset - Fashion-MNIST.\n",
        "\n",
        "**Challenge Requirements:**\n",
        "- Load Fashion-MNIST dataset (10 classes of clothing items)\n",
        "- Build a custom CNN architecture optimized for this dataset\n",
        "- Achieve at least 85% accuracy on the test set\n",
        "- Compare performance with different architectures\n",
        "- Analyze which clothing items are most/least accurately classified\n",
        "\n",
        "**Bonus Points:**\n",
        "- Implement transfer learning from MNIST-trained model\n",
        "- Create a confusion matrix analysis\n",
        "- Visualize learned features and filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercise 3 - Custom Dataset Challenge (Fashion-MNIST)\n",
        "print(\"=== Exercise 3: Fashion-MNIST Challenge ===\")\n",
        "\n",
        "# TODO: Level 1 - Load Fashion-MNIST dataset\n",
        "# fashion_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# fashion_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Fashion-MNIST classes\n",
        "fashion_classes = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "print(\"Fashion-MNIST Classes:\")\n",
        "for i, class_name in enumerate(fashion_classes):\n",
        "    print(f\"{i}: {class_name}\")\n",
        "\n",
        "# TODO: Level 2 - Design custom CNN for Fashion-MNIST\n",
        "class FashionCNN(nn.Module):\n",
        "    \"\"\"Custom CNN optimized for Fashion-MNIST\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        \n",
        "        # TODO: Design architecture specifically for Fashion-MNIST\n",
        "        # Consider: clothing items have different characteristics than digits\n",
        "        # - More complex textures and patterns\n",
        "        # - Different spatial relationships\n",
        "        # - May need more filters or different kernel sizes\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        pass\n",
        "\n",
        "# TODO: Level 3 - Train and evaluate model\n",
        "def train_fashion_model():\n",
        "    \"\"\"Train Fashion-MNIST model and return results\"\"\"\n",
        "    # TODO: Create model, optimizer, and training loop\n",
        "    # TODO: Implement early stopping to prevent overfitting\n",
        "    # TODO: Track training progress and return final accuracy\n",
        "    pass\n",
        "\n",
        "# TODO: Level 4 - Transfer learning experiment\n",
        "def transfer_learning_experiment():\n",
        "    \"\"\"Use MNIST-trained model as starting point for Fashion-MNIST\"\"\"\n",
        "    # TODO: Load pre-trained MNIST model\n",
        "    # TODO: Modify final layer for 10 Fashion-MNIST classes\n",
        "    # TODO: Fine-tune on Fashion-MNIST dataset\n",
        "    # TODO: Compare with training from scratch\n",
        "    pass\n",
        "\n",
        "# TODO: Level 5 - Analysis and visualization\n",
        "def analyze_fashion_results():\n",
        "    \"\"\"Analyze Fashion-MNIST classification results\"\"\"\n",
        "    # TODO: Create confusion matrix\n",
        "    # TODO: Identify most/least accurately classified items\n",
        "    # TODO: Visualize misclassified examples\n",
        "    # TODO: Analyze which features the model learned\n",
        "    pass\n",
        "\n",
        "print(\"\\nExercise 3: Fashion-MNIST challenge framework ready!\")\n",
        "print(\"Implement the TODO sections to complete the challenge.\")\n",
        "print(\"Target: Achieve 85%+ accuracy on Fashion-MNIST test set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Key Takeaways and Industry Insights\n",
        "\n",
        "### Core Concepts Mastered\n",
        "\n",
        "**CNN Architecture Fundamentals:**\n",
        "1. **Convolutional Layers**: Learn spatial features through shared filters\n",
        "2. **Pooling Layers**: Reduce dimensionality while preserving important information\n",
        "3. **Activation Functions**: Introduce non-linearity (ReLU dominates modern architectures)\n",
        "4. **Fully Connected Layers**: Final classification based on learned features\n",
        "5. **Batch Normalization**: Stabilize training and enable higher learning rates\n",
        "6. **Dropout**: Prevent overfitting through random neuron deactivation\n",
        "\n",
        "**Training Best Practices:**\n",
        "1. **Data Preprocessing**: Normalization and augmentation are crucial\n",
        "2. **Learning Rate Scheduling**: Adaptive learning rates improve convergence\n",
        "3. **Early Stopping**: Prevent overfitting by monitoring validation loss\n",
        "4. **Gradient Clipping**: Prevent exploding gradients in deep networks\n",
        "5. **Model Checkpointing**: Save progress for resuming training\n",
        "\n",
        "**Architecture Design Principles:**\n",
        "1. **Start Simple**: Begin with basic architectures and add complexity gradually\n",
        "2. **Parameter Efficiency**: More parameters ≠ better performance\n",
        "3. **Feature Hierarchy**: Early layers detect edges, later layers detect objects\n",
        "4. **Spatial Invariance**: Pooling provides translation robustness\n",
        "5. **Channel Growth**: Gradually increase filter count through network depth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Industry Applications and Real-World Impact\n",
        "\n",
        "**Where CNNs Excel in Production:**\n",
        "\n",
        "| Industry | Application | CNN Architecture | Performance Impact |\n",
        "|----------|-------------|------------------|-------------------|\n",
        "| **Healthcare** | Medical Imaging (X-rays, MRI, CT) | ResNet, DenseNet | 95%+ accuracy in diagnosis |\n",
        "| **Autonomous Vehicles** | Object Detection, Lane Detection | YOLO, SSD, Faster R-CNN | Real-time processing |\n",
        "| **E-commerce** | Product Recognition, Visual Search | Inception, MobileNet | 90%+ search accuracy |\n",
        "| **Security** | Facial Recognition, Surveillance | FaceNet, ArcFace | Sub-second identification |\n",
        "| **Manufacturing** | Quality Control, Defect Detection | Custom CNNs | 99%+ defect detection |\n",
        "| **Agriculture** | Crop Monitoring, Disease Detection | EfficientNet, Vision Transformer | 85%+ disease accuracy |\n",
        "\n",
        "**Famous CNN Architectures and Their Impact:**\n",
        "\n",
        "1. **LeNet (1998)**: First successful CNN, handwritten digit recognition\n",
        "2. **AlexNet (2012)**: ImageNet breakthrough, started deep learning revolution\n",
        "3. **VGG (2014)**: Simple architecture, excellent feature extraction\n",
        "4. **ResNet (2015)**: Residual connections, enabled very deep networks (100+ layers)\n",
        "5. **Inception (2015)**: Multi-scale feature extraction, efficient computation\n",
        "6. **DenseNet (2017)**: Dense connections, parameter efficiency\n",
        "7. **EfficientNet (2019)**: Compound scaling, optimal accuracy/efficiency trade-off\n",
        "\n",
        "**Modern Trends in Computer Vision:**\n",
        "- **Vision Transformers**: Attention-based models challenging CNN dominance\n",
        "- **Efficient Architectures**: MobileNet, ShuffleNet for edge deployment\n",
        "- **Self-Supervised Learning**: Training without labeled data\n",
        "- **Neural Architecture Search**: AI designing AI architectures\n",
        "- **Federated Learning**: Training on distributed data without centralization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pro Tips for CNN Success\n",
        "\n",
        "**Development Workflow:**\n",
        "1. **Start with Pre-trained Models**: Use torchvision.models for quick prototyping\n",
        "2. **Progressive Complexity**: Begin simple, add layers/filters gradually\n",
        "3. **Monitor Training**: Use TensorBoard or similar for real-time visualization\n",
        "4. **Version Control**: Track experiments with MLflow or Weights & Biases\n",
        "5. **A/B Testing**: Compare architectures systematically\n",
        "\n",
        "**Common Pitfalls to Avoid:**\n",
        "1. **Overfitting**: Use dropout, batch norm, and data augmentation\n",
        "2. **Poor Data Quality**: Clean, balanced datasets are crucial\n",
        "3. **Inadequate Preprocessing**: Normalization and augmentation matter\n",
        "4. **Wrong Architecture**: Match complexity to data complexity\n",
        "5. **Ignoring Baselines**: Always compare against simple models\n",
        "\n",
        "**Performance Optimization:**\n",
        "1. **Mixed Precision Training**: Use FP16 for faster training\n",
        "2. **Data Loading**: Use multiple workers and pin_memory=True\n",
        "3. **Model Pruning**: Remove unnecessary parameters\n",
        "4. **Quantization**: Reduce precision for deployment\n",
        "5. **Knowledge Distillation**: Train smaller models from larger ones\n",
        "\n",
        "**Debugging Strategies:**\n",
        "1. **Visualize Filters**: See what features each layer learns\n",
        "2. **Feature Maps**: Understand how data flows through network\n",
        "3. **Gradient Analysis**: Check for vanishing/exploding gradients\n",
        "4. **Loss Curves**: Monitor training vs validation performance\n",
        "5. **Sample Predictions**: Examine misclassified examples\n",
        "\n",
        "### Career Growth and Next Steps\n",
        "\n",
        "**Recommended Learning Path:**\n",
        "1. **Advanced Architectures**: ResNet, DenseNet, EfficientNet\n",
        "2. **Object Detection**: YOLO, R-CNN, SSD\n",
        "3. **Semantic Segmentation**: U-Net, DeepLab, Mask R-CNN\n",
        "4. **Vision Transformers**: ViT, Swin Transformer, CLIP\n",
        "5. **Generative Models**: GANs, VAEs, Diffusion Models\n",
        "\n",
        "**Portfolio Projects:**\n",
        "1. **Medical Image Analysis**: X-ray classification, tumor detection\n",
        "2. **Real-time Object Detection**: Webcam-based detection system\n",
        "3. **Style Transfer**: Neural style transfer application\n",
        "4. **Image Super-Resolution**: Enhance low-resolution images\n",
        "5. **Multi-modal Learning**: Combine vision with text/audio\n",
        "\n",
        "**Certifications and Courses:**\n",
        "- **Google ML Engineer**: Cloud ML specialization\n",
        "- **AWS Computer Vision**: SageMaker and Rekognition\n",
        "- **NVIDIA DLI**: Deep Learning Institute courses\n",
        "- **Fast.ai**: Practical deep learning course\n",
        "- **CS231n Stanford**: Computer vision course (free online)\n",
        "\n",
        "**Industry Resources:**\n",
        "- **Papers with Code**: Latest research implementations\n",
        "- **Hugging Face**: Pre-trained models and datasets\n",
        "- **OpenCV**: Computer vision library\n",
        "- **Albumentations**: Advanced data augmentation\n",
        "- **Weights & Biases**: Experiment tracking platform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Congratulations on completing Day 17!**\n",
        "\n",
        "You've now mastered:\n",
        "- CNN architecture fundamentals and modern implementation techniques\n",
        "- Building CNNs from scratch with PyTorch\n",
        "- Training optimization and best practices\n",
        "- Advanced techniques: batch normalization, dropout, data augmentation\n",
        "- Model evaluation, visualization, and interpretation\n",
        "- Industry applications and real-world impact\n",
        "\n",
        "**You're now equipped with industry-level CNN skills that will set you apart in computer vision roles!**\n",
        "\n",
        "Tomorrow, we'll dive into Recurrent Neural Networks (RNNs) and LSTMs - the foundation of sequence modeling. You'll learn to build models that can understand and generate text, analyze time series data, and process sequential information with state-of-the-art performance.\n",
        "\n",
        "**Remember**: The CNN techniques you've learned today are used by ML engineers at Google, Facebook, Tesla, and other leading tech companies. You're now part of that elite group of practitioners who can build production-ready computer vision systems!\n",
        "\n",
        "---\n",
        "\n",
        "## 📫 Let's Connect\n",
        "- 💼 **LinkedIn:** [hashirahmed07](https://www.linkedin.com/in/hashirahmed07/)\n",
        "- 📧 **Email:** [Hashirahmad330@gmail.com](mailto:Hashirahmad330@gmail.com)\n",
        "- 🐙 **GitHub:** [CodeByHashir](https://github.com/CodeByHashir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
