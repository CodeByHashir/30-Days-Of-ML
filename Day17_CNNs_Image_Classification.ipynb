{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 17: Convolutional Neural Networks (CNNs) - Image Classification Mastery\n",
        "\n",
        "**Welcome to Day 17 of your ML journey!** Today we dive into one of the most revolutionary architectures in deep learning: **Convolutional Neural Networks (CNNs)**. Building on your solid PyTorch foundation from Day 16, you'll now learn to build models that can \"see\" and understand images with superhuman accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Master CNN architecture and build production-ready image classification systems using PyTorch.\n",
        "\n",
        "**Topics Covered:**\n",
        "- CNN architecture: convolution, pooling, and feature learning\n",
        "- Building CNNs from scratch with PyTorch\n",
        "- Image preprocessing and data augmentation\n",
        "- Training CNNs on MNIST and CIFAR-10 datasets\n",
        "- Feature visualization and model interpretation\n",
        "- Advanced techniques: batch normalization, dropout, residual connections\n",
        "- Transfer learning fundamentals\n",
        "- Real-world applications and industry best practices\n",
        "\n",
        "**Real-World Impact:** CNNs power everything from medical diagnosis to autonomous vehicles, social media filters to security systems. By the end of today, you'll understand the technology behind these applications and be able to build your own image recognition systems.\n",
        "\n",
        "**Prerequisites:** Solid understanding of PyTorch fundamentals (Day 16), neural network basics (Day 15), and Python programming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Concept Overview: Understanding CNNs\n",
        "\n",
        "### What are Convolutional Neural Networks?\n",
        "\n",
        "**Convolutional Neural Networks (CNNs)** are specialized neural networks designed to process data with a grid-like topology, such as images. They're inspired by the visual cortex of animals and are exceptionally effective at recognizing patterns in visual data.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"Images/Convolutional Neural Network.jpeg\" alt=\"Convolutional Neural Network Architecture showing input layer, convolutional layers, pooling layers, and fully connected layers\" width=\"600\" height=\"400\">\n",
        "    <br>\n",
        "    <em id=\"figure1\">Figure 1: CNN Architecture - From input image through convolutional layers, pooling, and fully connected layers to final classification</em>\n",
        "</div>\n",
        "\n",
        "**The Core Intuition:**\n",
        "Think of CNNs like a team of specialized detectives examining a crime scene photo. Each detective (filter) looks for specific clues (features) - one might focus on edges, another on textures, another on shapes. They work together to piece together the complete picture.\n",
        "\n",
        "**Why CNNs Excel at Images:**\n",
        "1. **Spatial Relationships**: Preserves the 2D structure of images\n",
        "2. **Parameter Sharing**: Same filters applied across the entire image\n",
        "3. **Translation Invariance**: Recognizes objects regardless of position\n",
        "4. **Hierarchical Learning**: Low-level features → High-level concepts\n",
        "\n",
        "**Real-World Applications:**\n",
        "- **Medical Imaging**: Detecting tumors, analyzing X-rays, diagnosing diseases\n",
        "- **Autonomous Vehicles**: Recognizing traffic signs, pedestrians, other vehicles\n",
        "- **Social Media**: Face recognition, content moderation, photo enhancement\n",
        "- **Security**: Surveillance systems, biometric authentication\n",
        "- **E-commerce**: Product recognition, visual search, quality control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN Building Blocks Explained\n",
        "\n",
        "The diagram above (<a href=\"#figure1\">Figure 1</a>) shows a complete CNN architecture in action. Let's walk through each component and see how they work together to process images:\n",
        "\n",
        "#### 1. **Convolutional Layers** (The Feature Detectors)\n",
        "As shown in the diagram, convolutional layers are the heart of CNNs. These layers apply filters (kernels) to detect features:\n",
        "\n",
        "**How Convolution Works (Visualized in the diagram):**\n",
        "- **Input Image**: The diagram shows a raw image entering the network\n",
        "- **Filter Application**: Small filters (e.g., 3×3) slide across the image, as illustrated by the convolution operation\n",
        "- **Feature Maps**: Each filter produces a feature map highlighting specific patterns\n",
        "- **Multiple Filters**: Notice how different filters detect different features (edges, textures, patterns)\n",
        "\n",
        "**Key Parameters (Visible in the architecture):**\n",
        "- **Filter Size**: Typically 3×3 or 5×5 (larger = more context)\n",
        "- **Stride**: How many pixels the filter moves (1 = every pixel, 2 = every other pixel)\n",
        "- **Padding**: Adding zeros around the image to preserve size\n",
        "- **Number of Filters**: More filters = more feature types detected (see the multiple feature maps in the diagram)\n",
        "\n",
        "#### 2. **Activation Functions** (The Non-linearity Injectors)\n",
        "Between convolutional layers, activation functions introduce non-linearity:\n",
        "- **ReLU (Rectified Linear Unit)**: Most common, f(x) = max(0, x)\n",
        "- **Leaky ReLU**: Fixes \"dying ReLU\" problem\n",
        "- **ELU**: Smooth alternative with better gradient flow\n",
        "\n",
        "*Note: In the diagram, activation functions are applied after each convolutional layer, though not explicitly shown.*\n",
        "\n",
        "#### 3. **Pooling Layers** (The Dimension Reducers)\n",
        "The diagram clearly shows pooling layers reducing spatial dimensions while preserving important information:\n",
        "- **Max Pooling**: Takes maximum value in each region (most common) - visible as the downsampling in the diagram\n",
        "- **Average Pooling**: Takes average value in each region\n",
        "- **Benefits**: Reduces overfitting, computational cost, and parameters (notice how the feature maps get smaller)\n",
        "\n",
        "#### 4. **Fully Connected Layers** (The Final Classifiers)\n",
        "The diagram shows the transition from 2D feature maps to 1D vectors for final classification:\n",
        "- **Flattening**: Feature maps are flattened into vectors (visible in the diagram)\n",
        "- **Dense Layers**: Perform final classification or regression\n",
        "- **Output**: Produces the final prediction (shown as the output layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Convolution Works: A Deep Dive\n",
        "\n",
        "**The Convolution Operation:**\n",
        "Convolution is a mathematical operation that combines two functions to produce a third function. In CNNs, we use discrete convolution:\n",
        "\n",
        "<div style=\"background:rgb(10, 10, 7); border-left: 4px solid #667eea; padding: 20px; margin: 20px 0; border-radius: 8px;\">\n",
        "    <h4 style=\"color: #667eea; margin-top: 0;\"> Mathematical Formula</h4>\n",
        "    <div style=\"background: rgb(231, 231, 55); padding: 15px; border-radius: 5px; text-align: center; font-family: monospace; font-size: 16px; border: rgb(0, 0, 0);\">\n",
        "        <strong style=\"color: black;\">Output[i,j] = Σ Σ Input[i+m, j+n] × Filter[m, n]</strong><br>\n",
        "        <span style=\"color:rgb(56, 240, 56); font-size: 14px;\">where m, n are filter dimensions</span>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "**Step-by-Step Process (<a href=\"#figure2\">Figure 2</a>):**\n",
        "1. **Place Filter**: Position the filter over a region of the input (red highlighted area)\n",
        "2. **Element-wise Multiply**: Multiply corresponding elements (yellow calculation box)\n",
        "3. **Sum Results**: Add all products together (shown in yellow box)\n",
        "4. **Store Output**: Place result in corresponding position of output (feature map)\n",
        "5. **Slide Filter**: Move filter to next position and repeat (sequential red highlights)\n",
        "\n",
        "**Feature Detection Examples:**\n",
        "- **Edge Detection**: Filters that detect horizontal, vertical, diagonal edges\n",
        "- **Texture Detection**: Filters that identify patterns like wood grain, fabric\n",
        "- **Shape Detection**: Filters that recognize circles, squares, triangles\n",
        "- **Color Patterns**: Filters that detect specific color combinations\n",
        "\n",
        "**Hierarchical Learning:**\n",
        "- **Layer 1**: Detects edges, corners, basic shapes\n",
        "- **Layer 2**: Combines edges into textures, simple shapes\n",
        "- **Layer 3**: Recognizes object parts (eyes, wheels, doors)\n",
        "- **Layer 4+**: Identifies complete objects (faces, cars, buildings)\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"Images/3×3 filter sliding across a 5×5 image.png\" \n",
        "         alt=\"Step-by-step convolution demonstration showing 3x3 filter sliding across input image\" \n",
        "         width=\"700\" height=\"500\">\n",
        "    <br>\n",
        "    <em id=\"figure2\">Figure 2: Convolution Operation - 3×3 filter sliding across input with element-wise multiplication and summation</em>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pooling and Dimensionality Reduction\n",
        "\n",
        "**Why Pooling is Essential:**\n",
        "Pooling layers serve multiple critical purposes in CNNs:\n",
        "\n",
        "1. **Dimensionality Reduction**: Reduces spatial size of feature maps\n",
        "2. **Translation Invariance**: Makes the network robust to small shifts\n",
        "3. **Computational Efficiency**: Reduces parameters and computation\n",
        "4. **Overfitting Prevention**: Acts as a form of regularization\n",
        "\n",
        "**Max Pooling (Most Common):**\n",
        "- Takes the maximum value in each pooling region\n",
        "- Preserves the strongest activation (most important feature)\n",
        "- Commonly uses 2×2 pooling with stride 2\n",
        "- Reduces spatial dimensions by half\n",
        "\n",
        "**Average Pooling:**\n",
        "- Takes the average value in each pooling region\n",
        "- Smoother output, less sensitive to outliers\n",
        "- Sometimes used in final layers for global pooling\n",
        "\n",
        "**Global Pooling:**\n",
        "- Reduces entire feature map to single value\n",
        "- Global Average Pooling (GAP) popular in modern architectures\n",
        "- Eliminates need for fully connected layers\n",
        "\n",
        "**Spatial Invariance Benefits:**\n",
        "- Object recognition regardless of exact position\n",
        "- Robustness to small translations and rotations\n",
        "- Better generalization to new data\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"Images/Max Pooling vs Average Pooling.png\" \n",
        "         alt=\"Step-by-step convolution demonstration showing 3x3 filter sliding across input image\" \n",
        "         width=\"700\" height=\"500\">\n",
        "    <br>\n",
        "    <em>Figure 2: Max Pooling Vs Average Pooling</em>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Code Demo: Building CNNs with PyTorch\n",
        "\n",
        "Let's dive into practical implementation! We'll start with a simple CNN and progressively build more sophisticated architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check PyTorch version and device availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
